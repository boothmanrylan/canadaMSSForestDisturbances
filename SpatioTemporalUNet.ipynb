{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMxpmL/J0uOnhIXqUVOX8JG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/canadaMSSForestDisturbances/blob/main/SpatioTemporalUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ],
      "metadata": {
        "id": "nK2Okp4_y0rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "EJ6FKRJtxVQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtIfE64DyuEr"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = \"api-project-269347469410\"\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet msslib"
      ],
      "metadata": {
        "id": "r7oEsn_0yISr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --quiet https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances\n",
        "from mss_forest_disturbances import constants, grid, preprocessing, model"
      ],
      "metadata": {
        "id": "h8n3VuivE2Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.tools import saved_model_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm"
      ],
      "metadata": {
        "id": "4lQ9lMihzB0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "g2ZY4Mub0Ay3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = f'gs://{constants.BUCKET}/scratch/test_export4/ecozone*/'\n",
        "\n",
        "TEST_PATTERN = os.path.join(BASE_PATH, '*-00000-of-*.tfrecord.gz')\n",
        "TRAIN_PATTERN = os.path.join(BASE_PATH, '*-000[0-9][1-9]-of*.tfrecord.gz')\n",
        "\n",
        "train_dataset, normalize_subset = dataset.build_dataset(\n",
        "    pattern=TRAIN_PATTERN,\n",
        "    parse_options=constants.DEFAULT_PARSE_OPTIONS,\n",
        "    train=True,\n",
        ")\n",
        "test_dataset = dataset.build_dataset(\n",
        "    pattern=TEST_PATTERN,\n",
        "    parse_options=constants.DEFAULT_PARSE_OPTIONS,\n",
        "    train=False\n",
        ")\n",
        "\n",
        "model = model.build_model(\n",
        "    normalization_subset=normalization_subset,\n",
        "    **constants.DEFAULT_MODEL_OPTIONS\n",
        ")\n",
        "\n",
        "RNG = tf.random.Generator.from_seed(42, alg=\"philox\")\n",
        "\n",
        "# AI Platform Hosting Config\n",
        "REGION = \"us-central1\"\n",
        "MODEL_DIR = f\"gs://{constants.BUCKET}/scratch/models/\"\n",
        "EEIFIED_DIR = f\"gs://{constants.BUCKET}/scratch/eeified_models/test_model_hosting/\"\n",
        "MODEL_NAME = \"test_model\"\n",
        "ENDPOINT_NAME = \"test_endpoint\"\n",
        "\n"
      ],
      "metadata": {
        "id": "KzYCNhqYzp5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model architecture explanation\n"
      ],
      "metadata": {
        "id": "1e9crRQ9idGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_erf(kernels, dilation_rates):\n",
        "    k = np.array(kernels)\n",
        "    d = np.array(dilation_rates)\n",
        "    ek = k + ((k - 1) * (d - 1))\n",
        "\n",
        "    erf = np.sum(ek) - (len(kernels) - 1)\n",
        "    return erf"
      ],
      "metadata": {
        "id": "UWiWKwO2rBF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_erf(KERNELS, DILATION_RATES)"
      ],
      "metadata": {
        "id": "DEOWpJQ4rj7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "9OymIyqazQe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint to save progress during training and for easier loading of the\n",
        "# model later on, but need to use model.save(...) for EEification\n",
        "checkpoint_path = os.path.join(MODEL_DIR, \"test\", \"checkpoints\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "\n",
        "low_importance = 1\n",
        "high_importance = 2\n",
        "disturbance_classes = [4, 5, 6, 7]\n",
        "class_weight = {\n",
        "    x: (high_importance if x in disturbance_classes else low_importance)\n",
        "    for x in range(NUM_OUTPUTS)\n",
        "}\n",
        "\n",
        "# model.load_weights(checkpoint_path)\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=20,\n",
        "    callbacks=[checkpoint],\n",
        "    class_weight=class_weight,\n",
        ")"
      ],
      "metadata": {
        "id": "B4yd3MWp1aDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Hosting"
      ],
      "metadata": {
        "id": "zz8luZfpKkcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai models delete {MODEL_NAME} --project={PROJECT_ID} --region={REGION}"
      ],
      "metadata": {
        "id": "oS99CcKRgeZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the model\n",
        "CONTAINER_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest'\n",
        "\n",
        "!gcloud ai models upload \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --artifact-uri={SAVED_MODEL_PATH} \\\n",
        "    --region={REGION} \\\n",
        "    --container-image-uri={CONTAINER_IMAGE} \\\n",
        "    --description={MODEL_NAME} \\\n",
        "    --display-name={MODEL_NAME} \\\n",
        "    --model-id={MODEL_NAME}"
      ],
      "metadata": {
        "id": "2bpzmusQgn97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create endpoint for model\n",
        "!gcloud ai endpoints create \\\n",
        "    --display-name={ENDPOINT_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --project={PROJECT_ID}"
      ],
      "metadata": {
        "id": "ga00cPcrhJiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy the model\n",
        "\n",
        "# may need to filter, if you have multiple of these\n",
        "ENDPOINT_ID = !gcloud ai endpoints list \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --region={REGION} \\\n",
        "    --format=\"value(ENDPOINT_ID.scope())\"\n",
        "ENDPOINT_ID = ENDPOINT_ID[-1]\n",
        "\n",
        "!gcloud ai endpoints deploy-model {ENDPOINT_ID} \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --region={REGION} \\\n",
        "    --model={MODEL_NAME} \\\n",
        "    --machine-type=n1-standard-8 \\\n",
        "    --accelerator=type=nvidia-tesla-t4,count=1 \\\n",
        "    --display-name={MODEL_NAME}"
      ],
      "metadata": {
        "id": "bXWJohcihVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Model Hosting Was Successful"
      ],
      "metadata": {
        "id": "YLkstzWvn-xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "bmEyy8xp-vQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances"
      ],
      "metadata": {
        "id": "6nJKi6wKoUPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet msslib\n",
        "!pip install --quiet geemap"
      ],
      "metadata": {
        "id": "y6CfXggRoqKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mss_forest_disturbances import data\n",
        "import geemap\n",
        "from msslib import msslib"
      ],
      "metadata": {
        "id": "m-0XNXBroCGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map"
      ],
      "metadata": {
        "id": "bbo86gq-o4vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aoi = Map.draw_features[0]\n",
        "year = 1990\n",
        "\n",
        "collection = msslib.getCol(\n",
        "    aoi=aoi.geometry(),\n",
        "    yearRange=[year, year],\n",
        "    doyRange=data.DOY_RANGE,\n",
        "    maxCloudCover=100\n",
        ")\n",
        "\n",
        "image = collection.sort('CLOUD_COVER').first()\n",
        "\n",
        "Map.addLayer(image, msslib.visDn2, \"Image\")"
      ],
      "metadata": {
        "id": "ezi8GlCho9HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecozone = ee.FeatureCollection(data.ECOZONES).filterBounds(aoi.geometry()).first()\n",
        "ecozone_id = ecozone.getNumber('ECOZONE_ID')\n",
        "prepared_image, target_label = data.prepare_image_for_export(image)\n",
        "prepared_image = prepared_image.set('ecozone', ecozone_id)"
      ],
      "metadata": {
        "id": "EclgR0Jypaf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint_path = os.path.join('projects', PROJECT_ID, 'locations', REGION, 'endpoints', ENDPOINT_ID)\n",
        "hosted_model = ee.Model.fromVertexAi(\n",
        "    endpoint=endpoint_path,\n",
        "    inputTileSize=(constants.PATCH_SIZE, constants.PATCH_SIZE),\n",
        "    inputOverlapSize=(constants.OVERLAP, constants.OVERLAP),\n",
        "    inputProperties=METADATA,\n",
        "    proj=data.get_default_projection(),\n",
        "    fixInputProj=True,\n",
        "    outputBands={\n",
        "        'label': {\n",
        "            'type': ee.PixelType.float(),\n",
        "            'dimensions': 1\n",
        "        },\n",
        "    },\n",
        "    maxPayloadBytes=3000000,\n",
        ")"
      ],
      "metadata": {
        "id": "DkI0H9mDqAEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = hosted_model.predictImage(prepared_image)\n",
        "\n",
        "task = ee.batch.Export.image.toAsset(\n",
        "    image=prediction,\n",
        "    description=\"test_vertex_ai_hosting\",\n",
        "    assetId=\"projects/api-project-269347469410/assets/rylan-mssforestdisturbances/scratch/test_vertex_ai_hosting\",\n",
        "    pyramidingPolicy={\".default\": \"mode\"},\n",
        "    region=image.geometry(),\n",
        "    scale=60,\n",
        "    crs=data.get_default_projection(),\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "Q9JwkwZRrUfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "* __not enough disturbances in exported data__\n",
        "* Add index to distinguish new harvest from old harvest\n",
        "    * red / ndvi\n",
        "    * need way to prove/argue that this is a useful spectral index\n",
        "* Add index to distinguish new burn scar from old burn scar\n",
        "* temporal model\n",
        "    * write code\n",
        "    * figure out how to export training data\n",
        "* Figure out how to run colab with a paid backend\n",
        "* Vertex AI hosted model called through earth engine exporting the result is very slow (24 minutes for one image) Batch export and running everything in google cloud is likely faster, but more expensive and for the next step we need to be able to look at pixels through time which will be more difficult outside of earth engine\n"
      ],
      "metadata": {
        "id": "KkQ0d_Q7eMhI"
      }
    }
  ]
}