{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgDwOdlivmJfBlnxxl47Fs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/canadaMSSForestDisturbances/blob/main/SpatioTemporalUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ],
      "metadata": {
        "id": "nK2Okp4_y0rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "EJ6FKRJtxVQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtIfE64DyuEr"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = \"api-project-269347469410\"\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.tools import saved_model_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm"
      ],
      "metadata": {
        "id": "4lQ9lMihzB0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "g2ZY4Mub0Ay3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_OUTPUTS = 10\n",
        "EXPORT_HEIGHT = 512\n",
        "EXPORT_WIDTH = 512\n",
        "HEIGHT = 128\n",
        "WIDTH = 128\n",
        "\n",
        "# TODO: DEM will make an uneven number of bands that doesn't split into current/past as the DEM wont have changed\n",
        "BANDS = [\n",
        "    'nir', 'red_edge', 'red', 'green',\n",
        "    'tca', 'ndvi',\n",
        "    'historical_nir', 'historical_red_edge', 'historical_red', 'historical_green',\n",
        "    'historical_tca', 'historical_ndvi'\n",
        "]\n",
        "METADATA = ['doy', 'ecozone'] # , 'lat', 'lon']\n",
        "NUM_INPUTS = len([b for b in BANDS if \"historical\" not in b])\n",
        "LABEL_BAND = 'label'\n",
        "\n",
        "IMAGE_INPUT_LAYER_NAME = 'image'\n",
        "ECOZONE_INPUT_LAYER_NAME = 'ecozone'\n",
        "DOY_INPUT_LAYER_NAME = 'doy'\n",
        "\n",
        "MAX_DOY = 110\n",
        "NUM_ECOZONES = 10  # there are only seven represented in the sanity test dataset\n",
        "\n",
        "# Data Config\n",
        "BUCKET = 'rylan-mssforestdisturbances'\n",
        "BASE_PATH = f'gs://{BUCKET}/scratch/test_export/ecozone*/'\n",
        "TEST_PATTERN = os.path.join(BASE_PATH, '*-00000-of-*.tfrecord.gz')\n",
        "TRAIN_PATTERN = os.path.join(BASE_PATH, '*-000[0-9][1-9]-of*.tfrecord.gz')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER = 100\n",
        "\n",
        "SUBSET_SIZE = 100\n",
        "\n",
        "# Model Config\n",
        "FILTERS = [32, 64, 128, 256]\n",
        "KERNELS = [7, 5, 3, 3]\n",
        "DILATION_RATES = [1, 1, 2, 4]\n",
        "UPSAMPLE_FILTERS = 3\n",
        "METADATA_FILTERS = 32\n",
        "OUTPUT_KERNEL = 3\n",
        "MODEL_CONFIG = list(zip(FILTERS, KERNELS, DILATION_RATES))\n",
        "\n",
        "TWO_DOWNSTACKS = True\n",
        "INCLUDE_HISTORICAL = True\n",
        "INCLUDE_METADATA = True\n",
        "\n",
        "DATA_AUGMENTATION = False\n",
        "\n",
        "if not (TWO_DOWNSTACKS and INCLUDE_HISTORICAL):\n",
        "    BANDS = [b for b in BANDS if \"historical\" not in b]\n",
        "\n",
        "RNG = tf.random.Generator.from_seed(42, alg=\"philox\")\n",
        "\n",
        "# AI Platform Hosting Config\n",
        "REGION = \"us-central1\"\n",
        "MODEL_DIR = f\"gs://{BUCKET}/scratch/models/\"\n",
        "EEIFIED_DIR = f\"gs://{BUCKET}/scratch/eeified_models/test_model_hosting/\"\n",
        "MODEL_NAME = \"test_model\"\n",
        "ENDPOINT_NAME = \"test_endpoint\""
      ],
      "metadata": {
        "id": "KzYCNhqYzp5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "SDNWi739r1gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FEATURES = {\n",
        "    b: tf.io.FixedLenFeature(\n",
        "        shape=(EXPORT_HEIGHT, EXPORT_WIDTH),\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    for b in BANDS\n",
        "}\n",
        "\n",
        "LABEL_FEATURES = {\n",
        "    LABEL_BAND: tf.io.FixedLenFeature(\n",
        "        shape=(EXPORT_HEIGHT, EXPORT_WIDTH),\n",
        "        dtype=tf.int64\n",
        "    )\n",
        "}\n",
        "\n",
        "METADATA_FEATURES = {\n",
        "    m: tf.io.FixedLenFeature(shape=1, dtype=tf.int64)\n",
        "    for m in METADATA\n",
        "}\n",
        "\n",
        "\n",
        "def parse(example):\n",
        "    x = tf.io.parse_single_example(example, IMAGE_FEATURES)\n",
        "    x = tf.stack([x[b] for b in BANDS], axis=-1)\n",
        "\n",
        "    y = tf.io.parse_single_example(example, LABEL_FEATURES)[LABEL_BAND]\n",
        "    y = tf.one_hot(y, NUM_OUTPUTS)\n",
        "\n",
        "    metadata = tf.io.parse_single_example(example, METADATA_FEATURES)\n",
        "    metadata = [metadata[m] for m in METADATA]\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        x = (x, *metadata)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def non_overlapping_crop(x, y):\n",
        "    assert EXPORT_HEIGHT % HEIGHT == 0\n",
        "    assert EXPORT_WIDTH % WIDTH == 0\n",
        "\n",
        "    def _crop(tensor):\n",
        "        \"\"\" based on https://stackoverflow.com/a/31530106\n",
        "        \"\"\"\n",
        "        tensor = tf.reshape(\n",
        "            tensor,\n",
        "            (EXPORT_HEIGHT // HEIGHT, HEIGHT, EXPORT_WIDTH // WIDTH, WIDTH, -1)\n",
        "        )\n",
        "        cropped = tf.experimental.numpy.swapaxes(tensor, 1, 2)\n",
        "\n",
        "        num_blocks = (EXPORT_HEIGHT // HEIGHT) * (EXPORT_WIDTH // WIDTH)\n",
        "        cropped = tf.reshape(cropped, (num_blocks, HEIGHT, WIDTH, -1))\n",
        "        return tf.data.Dataset.from_tensor_slices(cropped)\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        metadata = [\n",
        "            tf.data.Dataset.from_tensor_slices(m).repeat()\n",
        "            for m in x[1:]\n",
        "        ]\n",
        "        x = x[0]\n",
        "\n",
        "    x = _crop(x)\n",
        "    y = _crop(y)\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        x = tf.data.Dataset.zip((x, *metadata))\n",
        "\n",
        "    return tf.data.Dataset.zip((x, y))\n",
        "\n",
        "\n",
        "def _apply_fn_to_xy(x, y, func):\n",
        "    if INCLUDE_METADATA:\n",
        "        metadata = x[1:]\n",
        "        x = x[0]\n",
        "\n",
        "    y_shape = tf.shape(y)\n",
        "    if len(y_shape) == 2:  # add temporary channel dimension\n",
        "        y = tf.reshape(y, y_shape + (1,))\n",
        "        num_y_bands = 1\n",
        "    else:\n",
        "        num_y_bands = y_shape[-1]\n",
        "\n",
        "    y_type = y.dtype\n",
        "    desired_type = x.dtype\n",
        "    y = tf.cast(y, desired_type)\n",
        "\n",
        "    xy = tf.concat([x, y], -1)\n",
        "\n",
        "    xy = func(xy)\n",
        "\n",
        "    x = xy[:, :, :-num_y_bands]\n",
        "\n",
        "    y = tf.squeeze(tf.cast(xy[:, :, -num_y_bands:], y_type))\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        x = (x, *metadata)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def crop(x, y, seed):\n",
        "    y_shape = tf.shape(y)\n",
        "    if len(y_shape) == 2:  # add temporary channel dimension\n",
        "        y = tf.reshape(y, y_shape + (1,))\n",
        "        num_y_bands = 1\n",
        "    else:\n",
        "        num_y_bands = y_shape[-1]\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        num_x_bands = tf.shape(x[0])[-1]\n",
        "    else:\n",
        "        num_x_bands = tf.shape(x)[-1]\n",
        "\n",
        "    target_shape = (HEIGHT, WIDTH, num_x_bands + num_y_bands)\n",
        "\n",
        "    def func(xy):\n",
        "        return tf.image.stateless_random_crop(xy, target_shape, seed=seed)\n",
        "\n",
        "    return _apply_fn_to_xy(x, y, func)\n",
        "\n",
        "\n",
        "def crop_wrapper(x, y):\n",
        "    seed = RNG.make_seeds(2)[0]\n",
        "    x, y = crop(x, y, seed)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "AUGMENTER = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.2, \"reflect\"),\n",
        "])\n",
        "\n",
        "def data_augmentation(x, y):\n",
        "    def func(xy):\n",
        "        return AUGMENTER(xy, training=True)\n",
        "\n",
        "    return _apply_fn_to_xy(x, y, func)\n",
        "\n",
        "\n",
        "def build_dataset(tfrecord_pattern, train=True):\n",
        "    tfrecords = tf.data.Dataset.list_files(tfrecord_pattern, shuffle=train)\n",
        "    dataset = tfrecords.interleave(\n",
        "        lambda x: tf.data.TFRecordDataset(x, compression_type='GZIP').map(parse, num_parallel_calls=1),\n",
        "        cycle_length=3 * NUM_ECOZONES,\n",
        "        block_length=BATCH_SIZE // 4,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "        deterministic=not train,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.cache()\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
        "        dataset = dataset.map(crop_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = dataset.flat_map(non_overlapping_crop)\n",
        "\n",
        "    if train:\n",
        "        subset = []\n",
        "        for x, y in dataset.take(math.ceil(SUBSET_SIZE / BATCH_SIZE)):\n",
        "            if INCLUDE_METADATA:\n",
        "                subset.append(x[0])\n",
        "            else:\n",
        "                subset.append(x)\n",
        "        subset = tf.concat(subset, axis=0)\n",
        "\n",
        "        if DATA_AUGMENTATION:  # do this after creating subset for normalization\n",
        "            dataset = dataset.map(\n",
        "                data_augmentation,\n",
        "                num_parallel_calls=tf.data.AUTOTUNE\n",
        "            )\n",
        "\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if train:\n",
        "        return dataset, subset\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset, normalize_subset = build_dataset(\n",
        "    TRAIN_PATTERN,\n",
        "    train=True,\n",
        ")\n",
        "test_dataset = build_dataset(\n",
        "    TEST_PATTERN,\n",
        "    train=False,\n",
        ")"
      ],
      "metadata": {
        "id": "hk_KyWjMr3x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spatial Model\n"
      ],
      "metadata": {
        "id": "T2T_wwEYzK0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalFusion(tf.keras.layers.Layer):\n",
        "    \"\"\" Change detection layer.\n",
        "\n",
        "    Based on Late Fusion from Maretto et al. 2021 10.1109/LGRS.2020.2986407\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=(1, 1),\n",
        "            padding=\"same\",\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "\n",
        "    def call(self, input1, input2):\n",
        "        x = tf.concat([input1, input2], -1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownSample(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.separable_conv2d_1 = tf.keras.layers.SeparableConv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            padding=\"same\",\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.separable_conv2d_1(x)\n",
        "        x = self.batch_norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.transposed_conv2d_1 = tf.keras.layers.Conv2DTranspose(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.transposed_conv2d_1(x)\n",
        "        x = self.batch_norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MetadataBias(tf.keras.layers.Layer):\n",
        "    \"\"\" Layer to include scalar metadata in a fully convolutional network.\n",
        "\n",
        "    Based on LSENet from Xie, Guo, and Dong 2022 10.1109/TGRS.2022.3176635\n",
        "\n",
        "    x += Dense(Concat([Dense(GlobalAvgPool(x)), Embedding(scalars)]))\n",
        "    \"\"\"\n",
        "    def __init__(self, num_outputs, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_outputs = num_outputs\n",
        "        self.num_inputs = self.num_outputs // 3\n",
        "\n",
        "        self.doy_embedding = tf.keras.layers.Embedding(\n",
        "            MAX_DOY,\n",
        "            self.num_inputs\n",
        "        )\n",
        "        self.ecozone_embedding = tf.keras.layers.Embedding(\n",
        "            NUM_ECOZONES,\n",
        "            self.num_inputs\n",
        "        )\n",
        "\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.dense1 = tf.keras.layers.Dense(self.num_inputs)\n",
        "        self.dense2 = tf.keras.layers.Dense(self.num_outputs)\n",
        "\n",
        "    def call(self, x, doy, ecozone):\n",
        "        doy_embedding = self.doy_embedding(doy)[:, 0]\n",
        "        ecozone_embedding = self.ecozone_embedding(ecozone)[:, 0]\n",
        "\n",
        "        pooled_x = self.pool(x)\n",
        "        pooled_x = self.dense1(pooled_x)\n",
        "\n",
        "        metadata = tf.concat(\n",
        "            [pooled_x, doy_embedding, ecozone_embedding],\n",
        "            axis=-1\n",
        "        )\n",
        "        metadata = self.dense2(metadata)\n",
        "        metadata = tf.reshape(metadata, (-1, 1, 1, self.num_outputs))\n",
        "\n",
        "        return x + metadata"
      ],
      "metadata": {
        "id": "SnxOLjLIaxZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_two_downstack_model(subset):\n",
        "    normalizer = tf.keras.layers.Normalization()\n",
        "    normalizer.adapt(subset)\n",
        "\n",
        "    input_layer = tf.keras.layers.Input(\n",
        "        shape=(HEIGHT, WIDTH, 2 * NUM_INPUTS),\n",
        "        name=IMAGE_INPUT_LAYER_NAME,\n",
        "    )\n",
        "    x = normalizer(input_layer)\n",
        "\n",
        "    x1 = x[:, :, :, :NUM_INPUTS]\n",
        "    x2 = x[:, :, :, NUM_INPUTS:]\n",
        "\n",
        "    down_stack_1 = [DownSample(*config) for config in MODEL_CONFIG]\n",
        "    down_stack_2 = [DownSample(*config) for config in MODEL_CONFIG]\n",
        "    up_stack = [UpSample(f, UPSAMPLE_FILTERS) for f in reversed(FILTERS)]\n",
        "\n",
        "    skips = []\n",
        "    for i, (down1, down2) in enumerate(zip(down_stack_1, down_stack_2)):\n",
        "        x1 = down1(x1)\n",
        "        x2 = down2(x2)\n",
        "        x = TemporalFusion(FILTERS[i])(x1, x2)\n",
        "        skips.append(x)\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        doy_input = tf.keras.layers.Input(\n",
        "            shape=1,\n",
        "            dtype=tf.int64,\n",
        "            name=DOY_INPUT_LAYER_NAME,\n",
        "        )\n",
        "        ecozone_input = tf.keras.layers.Input(\n",
        "            shape=1,\n",
        "            dtype=tf.int64,\n",
        "            name=ECOZONE_INPUT_LAYER_NAME,\n",
        "        )\n",
        "\n",
        "        metadata_bias = MetadataBias(FILTERS[-1])\n",
        "        x = metadata_bias(x, doy_input, ecozone_input)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(\n",
        "        NUM_OUTPUTS,\n",
        "        kernel_size=OUTPUT_KERNEL,\n",
        "        padding=\"same\",\n",
        "        activation=\"softmax\",\n",
        "    )(x)\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        inputs = [input_layer, doy_input, ecozone_input]\n",
        "    else:\n",
        "        inputs = input_layer\n",
        "\n",
        "    model = tf.keras.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_single_downstack_model(subset):\n",
        "    if INCLUDE_HISTORICAL:\n",
        "        shape = (HEIGHT, WIDTH, 2 * NUM_INPUTS)\n",
        "    else:\n",
        "        shape = (HEIGHT, WIDTH, NUM_INPUTS)\n",
        "\n",
        "    input_layer = tf.keras.layers.Input(\n",
        "        shape=shape,\n",
        "        name=IMAGE_INPUT_LAYER_NAME\n",
        "    )\n",
        "\n",
        "    image_normalizer = tf.keras.layers.Normalization()\n",
        "    image_normalizer.adapt(subset)\n",
        "\n",
        "    x = image_normalizer(input_layer)\n",
        "\n",
        "    down_stack = [DownSample(*config) for config in MODEL_CONFIG]\n",
        "    up_stack = [UpSample(f, UPSAMPLE_FILTERS) for f in reversed(FILTERS)]\n",
        "\n",
        "    skips = []\n",
        "    for i, down in enumerate(down_stack):\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        doy_input = tf.keras.layers.Input(\n",
        "            shape=1,\n",
        "            name=DOY_INPUT_LAYER_NAME,\n",
        "        )\n",
        "        ecozone_input = tf.keras.layers.Input(\n",
        "            shape=1,\n",
        "            name=ECOZONE_INPUT_LAYER_NAME,\n",
        "        )\n",
        "\n",
        "        metadata_bias = MetadataBias(METADATA_FILTERS, FILTERS[-1])\n",
        "        x = metadata_bias(x, doy_input, ecozone_input)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(\n",
        "        NUM_OUTPUTS,\n",
        "        kernel_size=OUTPUT_KERNEL,\n",
        "        padding=\"same\",\n",
        "        activation=\"softmax\",\n",
        "    )(x)\n",
        "\n",
        "    if INCLUDE_METADATA:\n",
        "        inputs = [input_layer, doy_input, ecozone_input]\n",
        "    else:\n",
        "        inputs = input_layer\n",
        "\n",
        "    model = tf.keras.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "if TWO_DOWNSTACKS:\n",
        "    model = build_two_downstack_model(normalize_subset)\n",
        "else:\n",
        "    model = build_single_downstack_model(normalize_subset)\n",
        "# tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "js_aab3ZZu4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Model"
      ],
      "metadata": {
        "id": "BRQiXeNpQAqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences=True,\n",
        "        )\n",
        "        self.lstm2 = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences=True,\n",
        "        )\n",
        "        self.lstm3 = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences=False,\n",
        "            return_state=True,\n",
        "        )\n",
        "\n",
        "    def call(self, x, initial_states=None):\n",
        "        initial_state = [None] * 3 if initial_state is None\n",
        "        x, state1 = self.lstm1(x, initial_state=initial_state[0])\n",
        "        x, state2 = self.lstm2(x, initial_state=initial_state[1])\n",
        "        x, state3 = self.lstm3(x, initial_state=initial_state[2])\n",
        "        return x, [state1, state2, state3]"
      ],
      "metadata": {
        "id": "oHKliI_cRCm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_temporal_model(units, num_inputs, num_outputs):\n",
        "    lookback_input = tf.keras.layers.Input(shape=(None, num_inputs))\n",
        "    target_input = tf.keras.layers.Input(shape=(None, num_inputs))\n",
        "    lookahead_input = tf.keras.layers.Input(shape=(None, num_inputs))\n",
        "\n",
        "    lookback, states = RecurrentBlock(units)(lookback_input)\n",
        "    target, states = RecurrentBlock(units)target_input, initial_states=states)\n",
        "    lookahead, _ = RecurrentBlock(units)(lookahead_input, initial_state=states)\n",
        "\n",
        "    x = tf.concat([lookback, target, lookahead])\n",
        "\n",
        "    x = tf.Dense(\n",
        "        num_outputs,\n",
        "        activation=\"softmax\" if num_outputs > 1 else \"sigmoid\",\n",
        "    )(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[lookback_input, target_input, lookahead_input], outputs=x)\n",
        "    return model\n",
        "\n",
        "temporal_model = build_temporal_model(64, 16, 3)\n",
        "tf.keras.utils.plot(temporal_model)"
      ],
      "metadata": {
        "id": "MGMnl6GJQFUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "9OymIyqazQe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng()\n",
        "\n",
        "size = 50\n",
        "data_A = rng.normal(0, 1, (size, HEIGHT, WIDTH, NUM_INPUTS))\n",
        "data_B = rng.normal(0, 1, (size, HEIGHT, WIDTH, NUM_INPUTS))\n",
        "\n",
        "labels = tf.one_hot(rng.integers(0, NUM_OUTPUTS, (size, HEIGHT, WIDTH)), NUM_OUTPUTS)"
      ],
      "metadata": {
        "id": "UNfBGSMXzTiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint to save progress during training and for easier loading of the\n",
        "# model later on, but need to use model.save(...) for EEification\n",
        "checkpoint_path = os.path.join(MODEL_DIR, \"test\", \"checkpoints\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "\n",
        "# model.fit(\n",
        "#     train_dataset,\n",
        "#     steps_per_epoch=50,\n",
        "#     epochs=10,\n",
        "#     callbacks=[checkpoint],\n",
        "# )\n",
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "B4yd3MWp1aDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n"
      ],
      "metadata": {
        "id": "uoLJqiUwoJ9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_colours = [\"white\", \"black\", \"gold\", \"darkCyan\", \"darkOrange\", \"red\",\n",
        "                 \"orchid\", \"purple\", \"cornsilk\", \"dimGrey\"]\n",
        "CLASS_LIST = [\"None\", \"Non-Forest\", \"Forest\", \"Water\", \"Previous Burn\",\n",
        "              \"Burn\", \"Previous Harvest\", \"Harvest\", \"Cloud\", \"Cloud Shadow\"]\n",
        "cmap = ListedColormap(class_colours, NUM_OUTPUTS)\n",
        "norm = BoundaryNorm(np.arange(NUM_OUTPUTS + 1), NUM_OUTPUTS)\n",
        "\n",
        "def _plot_x(x, axes, i, j):\n",
        "    x = tf.gather(x, (0, 1, 2), axis=-1)\n",
        "    std = np.std(x)\n",
        "    vmin = np.mean(x) - std\n",
        "    vmax = np.mean(x) + std\n",
        "    axes[i, j].imshow(x, vmin=vmin, vmax=vmax)\n",
        "\n",
        "def _plot_y(y, axes, i, j):\n",
        "    y = np.squeeze(np.argmax(y, axis=-1))\n",
        "    axes[i, j].imshow(y, cmap=cmap, norm=norm)"
      ],
      "metadata": {
        "id": "EcqBZ3UnZPQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to verify that the cropping does what we expect\n",
        "def crop_visualizer(pattern, count=10, deterministic_crop=False):\n",
        "    files = tf.data.Dataset.list_files(pattern, shuffle=False)\n",
        "    raw_dataset = tf.data.TFRecordDataset(files, compression_type='GZIP')\n",
        "    dataset = raw_dataset.map(parse)\n",
        "    dataset = dataset.cache()\n",
        "\n",
        "    size = 6\n",
        "    rgb_indices = (0, 1, 2)\n",
        "\n",
        "    if deterministic_crop:\n",
        "        cropped_dataset = dataset.flat_map(non_overlapping_crop)\n",
        "        cropped_dataset = cropped_dataset.take(4 * count)\n",
        "        fig, axes = plt.subplots(count, 10, figsize=(10 * size, count * size))\n",
        "    else:\n",
        "        cropped_dataset = dataset.map(crop_wrapper)\n",
        "        cropped_dataset = cropped_dataset.take(count)\n",
        "        fig, axes = plt.subplots(count, 4, figsize=(4 * size, count * size))\n",
        "\n",
        "    dataset = dataset.take(count)\n",
        "\n",
        "    if deterministic_crop:\n",
        "        for i, (x, y) in enumerate(dataset):\n",
        "            if INCLUDE_METADATA:\n",
        "                x = x[0]\n",
        "            _plot_x(x, axes, i, 0)\n",
        "            _plot_y(y, axes, i, 5)\n",
        "\n",
        "        for i, (x, y) in enumerate(cropped_dataset):\n",
        "            if INCLUDE_METADATA:\n",
        "                x = x[0]\n",
        "            _plot_x(x, axes, i // 4, 1 + (i % 4))\n",
        "            _plot_y(y, axes, i // 4, 6 + (i % 4))\n",
        "    else:\n",
        "        for i, (x, y) in enumerate(dataset):\n",
        "            if INCLUDE_METADATA:\n",
        "                x = x[0]\n",
        "            _plot_x(x, axes, i, 0)\n",
        "            _plot_y(y, axes, i, 2)\n",
        "\n",
        "        for i, (x, y) in enumerate(cropped_dataset):\n",
        "            if INCLUDE_METADATA:\n",
        "                x = x[0]\n",
        "            _plot_x(x, axes, i, 1)\n",
        "            _plot_y(y, axes, i, 3)\n",
        "\n",
        "# crop_visualizer(TRAIN_PATTERN, deterministic_crop=True)\n",
        "# crop_visualizer(TRAIN_PATTERN, deterministic_crop=False)"
      ],
      "metadata": {
        "id": "agchBqbDUn-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to verify that data augmentation does what we intend\n",
        "def data_augmentation_visualizer(pattern, count=10):\n",
        "    files = tf.data.Dataset.list_files(pattern, shuffle=False)\n",
        "    raw_dataset = tf.data.TFRecordDataset(files, compression_type='GZIP')\n",
        "    dataset = raw_dataset.map(parse)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.take(count)\n",
        "\n",
        "    size = 6\n",
        "    rgb_indices = (0, 1, 2)\n",
        "\n",
        "    augmented_dataset = dataset.map(data_augmentation)\n",
        "\n",
        "    fig, axes = plt.subplots(count, 4, figsize=(4 * size, count * size))\n",
        "\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if INCLUDE_METADATA:\n",
        "            x = x[0]\n",
        "        _plot_x(x, axes, i, 0)\n",
        "        _plot_y(y, axes, i, 1)\n",
        "\n",
        "    for i, (x, y) in enumerate(augmented_dataset):\n",
        "        if INCLUDE_METADATA:\n",
        "            x = x[0]\n",
        "        _plot_x(x, axes, i, 2)\n",
        "        _plot_y(y, axes, i, 3)\n",
        "\n",
        "data_augmentation_visualizer(TRAIN_PATTERN, 25)"
      ],
      "metadata": {
        "id": "KSjGIKDFqvIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualizer(dataset, model=None, count=10):\n",
        "    rgb_indices = [0, 1, 2]\n",
        "    historical_rgb_indices = [6, 7, 8]\n",
        "\n",
        "    data = dataset.unbatch()\n",
        "\n",
        "    num = 3 if model is None else 4\n",
        "    size = 10\n",
        "    fig, axes = plt.subplots(count, num, figsize=(num * size, count * size))\n",
        "\n",
        "    def plot_row(x, hx, y, model_output, index):\n",
        "        vmin_x = np.mean(x) - (0.5 * np.std(x))\n",
        "        vmax_x = np.mean(x) + (0.5 * np.std(x))\n",
        "        vmin_hx = np.mean(hx) - (0.5 * np.std(hx))\n",
        "        vmax_hx = np.mean(hx) + (0.5 * np.std(hx))\n",
        "        axes[index, 0].imshow(hx, vmin=vmin_hx, vmax=vmax_hx)\n",
        "        axes[index, 1].imshow(x, vmin=vmin_x, vmax=vmax_x)\n",
        "        y = np.argmax(y, axis=-1)\n",
        "        axes[index, 2].imshow(y, cmap=cmap, norm=norm)\n",
        "        if model_output is not None:\n",
        "            model_output = np.squeeze(np.argmax(model_output, axis=-1))\n",
        "            axes[index, 3].imshow(model_output, cmap=cmap, norm=norm)\n",
        "\n",
        "    for i, (x, y) in enumerate(data.take(count)):\n",
        "        if INCLUDE_METADATA:\n",
        "            _x = tf.expand_dims(x[0], axis=0)\n",
        "            metadata = [tf.expand_dims(m, axis=0) for m in x[1:]]\n",
        "            _x = [_x, *metadata]\n",
        "        else:\n",
        "            _x = tf.expand_dims(x, axis=0)\n",
        "\n",
        "        model_output = None if model is None else model(_x)\n",
        "\n",
        "        if INCLUDE_METADATA:\n",
        "            x = x[0]\n",
        "\n",
        "        x_rgb = tf.gather(x, rgb_indices, axis=-1)\n",
        "        hx_rgb = tf.gather(x, historical_rgb_indices, axis=-1)\n",
        "\n",
        "        plot_row(x_rgb, hx_rgb, y, model_output, i)\n",
        "\n",
        "\n",
        "visualizer(train_dataset, model=model, count=25)"
      ],
      "metadata": {
        "id": "UHCVQ_b0ZMEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment"
      ],
      "metadata": {
        "id": "DYUHQs_lIE8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_confusion_matrix(model, dataset):\n",
        "    complete_confusion_matrix = tf.zeros(\n",
        "        (NUM_OUTPUTS, NUM_OUTPUTS),\n",
        "        dtype=tf.int32\n",
        "    )\n",
        "\n",
        "    for x, y in dataset.take(10):\n",
        "        y_prime = model(x)\n",
        "\n",
        "        current_confusion_matrix = tf.math.confusion_matrix(\n",
        "            labels=tf.reshape(tf.argmax(y, -1), [-1]),\n",
        "            predictions=tf.reshape(tf.argmax(y_prime, -1), [-1]),\n",
        "            num_classes=NUM_OUTPUTS,\n",
        "        )\n",
        "\n",
        "        complete_confusion_matrix += current_confusion_matrix\n",
        "\n",
        "    return complete_confusion_matrix\n",
        "\n",
        "def label_confusion_matrix(confusion_matrix, class_labels):\n",
        "    confusion_matrix_df = pd.DataFrame(\n",
        "        confusion_matrix,\n",
        "        index=['True ' + label for label in class_labels],\n",
        "        columns=['Pred ' + label for label in class_labels]\n",
        "    )\n",
        "\n",
        "    return confusion_matrix_df\n",
        "\n",
        "cm = build_confusion_matrix(model, test_dataset)\n",
        "label_confusion_matrix(cm, CLASS_LIST)"
      ],
      "metadata": {
        "id": "PmX1y2XBIH8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EEification\n",
        "\n",
        "See: https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_Vertex_AI.ipynb\n",
        "\n",
        "And also: https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_tree_counting_model.ipynb\n"
      ],
      "metadata": {
        "id": "wpUn0e7niLzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing(tf.keras.layers.Layer):\n",
        "    \"\"\" Based on:\n",
        "    https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_Vertex_AI.ipynb\n",
        "\n",
        "    Stacks and reshapes input tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, features_dict):\n",
        "        # (None, 1, 1, 1) -> (None, 1, 1, P)\n",
        "        image = tf.concat([features_dict[b] for b in BANDS], axis=-1, name='image')\n",
        "        if INCLUDE_METADATA:\n",
        "            metadata = [features_dict[m] for m in METADATA]\n",
        "            return (image, *metadata)\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "class WrappedModel(tf.keras.Model):\n",
        "    \"\"\" Based on:\n",
        "    https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_Vertex_AI.ipynb\n",
        "\n",
        "    Wraps a given model in Preprocessing Layer\n",
        "    \"\"\"\n",
        "    def __init__(self, model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.preprocessing = Preprocessing()\n",
        "        self.model = model\n",
        "\n",
        "    def call(self, features_dict):\n",
        "        x = self.preprocessing(features_dict)\n",
        "        return self.model(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config"
      ],
      "metadata": {
        "id": "M78ocR8fJqZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeSerializeInput(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs_dict):\n",
        "        serialized_dict = {\n",
        "            k: tf.map_fn(\n",
        "                lambda x: tf.io.parse_tensor(x, tf.float32),\n",
        "                tf.io.decode_base64(v),\n",
        "                fn_output_signature=tf.float32\n",
        "            )\n",
        "            for (k, v) in inputs_dict.items()\n",
        "            if k in BANDS\n",
        "        }\n",
        "\n",
        "        # scalar metadata should be parsed as int64 not float\n",
        "        for (k, v) in inputs_dict.items():\n",
        "            if k not in BANDS:\n",
        "                serialized_dict[k] = tf.map_fn(\n",
        "                    lambda x: tf.io.parse_tensor(x, tf.int64),\n",
        "                    tf.io.decode_base64(v),\n",
        "                    fn_output_signature=tf.int64\n",
        "                )\n",
        "\n",
        "        return serialized_dict\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class ReSerializeOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, output_tensor):\n",
        "        return tf.map_fn(\n",
        "            lambda x: tf.io.encode_base64(tf.io.serialize_tensor(x)),\n",
        "            output_tensor,\n",
        "            fn_output_signature=tf.string\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "input_deserializer = DeSerializeInput()\n",
        "output_reserializer = ReSerializeOutput()\n",
        "\n",
        "serialized_inputs = {\n",
        "    x: tf.keras.Input(shape=[], dtype='string', name=x)\n",
        "    for x in (BANDS + METADATA if INCLUDE_METADATA else BANDS)\n",
        "}\n",
        "\n",
        "model.load_weights(os.path.join(MODEL_DIR, \"test\", \"checkpoints\"))\n",
        "\n",
        "wrapped_model = WrappedModel(model)\n",
        "updated_model_input = input_deserializer(serialized_inputs)\n",
        "updated_model = wrapped_model(updated_model_input)\n",
        "updated_model = output_reserializer(updated_model)\n",
        "updated_model = tf.keras.Model(serialized_inputs, updated_model)\n",
        "\n",
        "SAVED_MODEL_PATH = os.path.join(MODEL_DIR, \"test\", \"full_model\")\n",
        "\n",
        "!gsutil rm -rf {SAVED_MODEL_PATH}\n",
        "updated_model.save(SAVED_MODEL_PATH)"
      ],
      "metadata": {
        "id": "-lregtxidHCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai models delete {MODEL_NAME} --project={PROJECT_ID} --region={REGION}"
      ],
      "metadata": {
        "id": "oS99CcKRgeZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the model\n",
        "CONTAINER_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest'\n",
        "\n",
        "!gcloud ai models upload \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --artifact-uri={SAVED_MODEL_PATH} \\\n",
        "    --region={REGION} \\\n",
        "    --container-image-uri={CONTAINER_IMAGE} \\\n",
        "    --description={MODEL_NAME} \\\n",
        "    --display-name={MODEL_NAME} \\\n",
        "    --model-id={MODEL_NAME}"
      ],
      "metadata": {
        "id": "2bpzmusQgn97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create endpoint for model\n",
        "!gcloud ai endpoints create \\\n",
        "    --display-name={ENDPOINT_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --project={PROJECT_ID}"
      ],
      "metadata": {
        "id": "ga00cPcrhJiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy the model\n",
        "\n",
        "# may need to filter, if you have multiple of these\n",
        "ENDPOINT_ID = !gcloud ai endpoints list \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --region={REGION} \\\n",
        "    --format=\"value(ENDPOINT_ID.scope())\"\n",
        "ENDPOINT_ID = ENDPOINT_ID[-1]\n",
        "\n",
        "!gcloud ai endpoints deploy-model {ENDPOINT_ID} \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --region={REGION} \\\n",
        "    --model={MODEL_NAME} \\\n",
        "    --machine-type=n1-standard-8 \\\n",
        "    --accelerator=type=nvidia-tesla-t4,count=1 \\\n",
        "    --display-name={MODEL_NAME}"
      ],
      "metadata": {
        "id": "bXWJohcihVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Model Hosting Was Successful"
      ],
      "metadata": {
        "id": "YLkstzWvn-xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "bmEyy8xp-vQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances"
      ],
      "metadata": {
        "id": "6nJKi6wKoUPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet msslib\n",
        "!pip install --quiet geemap"
      ],
      "metadata": {
        "id": "y6CfXggRoqKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mss_forest_disturbances import data\n",
        "import geemap\n",
        "from msslib import msslib"
      ],
      "metadata": {
        "id": "m-0XNXBroCGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map"
      ],
      "metadata": {
        "id": "bbo86gq-o4vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aoi = Map.draw_features[0]\n",
        "year = 1990\n",
        "\n",
        "collection = msslib.getCol(\n",
        "    aoi=aoi.geometry(),\n",
        "    yearRange=[year, year],\n",
        "    doyRange=data.DOY_RANGE,\n",
        "    maxCloudCover=100\n",
        ")\n",
        "\n",
        "image = collection.sort('CLOUD_COVER').first()\n",
        "\n",
        "Map.addLayer(image, msslib.visDn2, \"Image\")"
      ],
      "metadata": {
        "id": "ezi8GlCho9HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecozone = ee.FeatureCollection(data.ECOZONES).filterBounds(aoi.geometry()).first()\n",
        "ecozone_id = ecozone.getNumber('ECOZONE_ID')\n",
        "prepared_image, target_label = data.prepare_image_for_export(image)\n",
        "prepared_image = prepared_image.set('ecozone', ecozone_id)"
      ],
      "metadata": {
        "id": "EclgR0Jypaf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint_path = os.path.join('projects', PROJECT_ID, 'locations', REGION, 'endpoints', ENDPOINT_ID)\n",
        "hosted_model = ee.Model.fromVertexAi(\n",
        "    endpoint=endpoint_path,\n",
        "    inputTileSize=(HEIGHT, WIDTH),\n",
        "    inputOverlapSize=(16, 16),\n",
        "    inputProperties=METADATA,\n",
        "    proj=data.get_default_projection(),\n",
        "    fixInputProj=True,\n",
        "    outputBands={\n",
        "        'label': {\n",
        "            'type': ee.PixelType.float(),\n",
        "            'dimensions': 1\n",
        "        },\n",
        "    },\n",
        "    maxPayloadBytes=3000000,\n",
        ")"
      ],
      "metadata": {
        "id": "DkI0H9mDqAEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = hosted_model.predictImage(prepared_image)\n",
        "\n",
        "task = ee.batch.Export.image.toAsset(\n",
        "    image=prediction,\n",
        "    description=\"test_vertex_ai_hosting\",\n",
        "    assetId=\"projects/api-project-269347469410/assets/rylan-mssforestdisturbances/scratch/test_vertex_ai_hosting\",\n",
        "    pyramidingPolicy={\".default\": \"mode\"},\n",
        "    region=image.geometry(),\n",
        "    scale=60,\n",
        "    crs=data.get_default_projection(),\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "Q9JwkwZRrUfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "* set up assessment code\n",
        "* __not enough disturbances in exported data__\n",
        "* Add Digital Elevation Model band back to export\n",
        "* Add index to distinguish new harvest from old harvest\n",
        "    * red / ndvi\n",
        "    * need way to prove/argue that this is a useful spectral index\n",
        "* Add index to distinguish new burn scar from old burn scar\n",
        "* temporal model\n",
        "    * write code\n",
        "    * figure out how to export training data\n",
        "* Figure out how to run colab with a paid backend\n",
        "* Vertex AI hosted model called through earth engine exporting the result is very slow (24 minutes for one image) Batch export and running everything in google cloud is likely faster, but more expensive and for the next step we need to be able to look at pixels through time which will be more difficult outside of earth engine\n"
      ],
      "metadata": {
        "id": "KkQ0d_Q7eMhI"
      }
    }
  ]
}