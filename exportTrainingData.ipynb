{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMmgwerZ5yJBOJ4HMO775pd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/canadaMSSForestDisturbances/blob/main/exportTrainingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lHfGEmw8-PKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "UHuSQFE_TMlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade pip\n",
        "!pip install --quiet \"apache-beam[gcp]==2.46.0\"\n",
        "!pip install --quiet geemap"
      ],
      "metadata": {
        "id": "znBYs0werBFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMPH5MSEaA9_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import itertools\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "from google.api_core import retry\n",
        "\n",
        "import requests\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "import ee\n",
        "import geemap\n",
        "import geopandas\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = 'api-project-269347469410'\n",
        "BUCKET = 'gs://rylan-mssforestdisturbances/'\n",
        "LOCATION = 'us-central1'\n",
        "\n",
        "HIGH_VOLUME_ENDPOINT = 'https://earthengine-highvolume.googleapis.com'\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT\n",
        "!gcloud config set project {PROJECT}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT, opt_url=HIGH_VOLUME_ENDPOINT)"
      ],
      "metadata": {
        "id": "tC8CmOUDa_sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone and install msslib\n",
        "!git clone --quiet https://github.com/boothmanrylan/msslib.git\n",
        "%cd msslib\n",
        "!pip install --quiet .\n",
        "%cd ..\n",
        "\n",
        "from msslib import msslib\n",
        "\n",
        "!git clone --quiet https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances\n",
        "from mss_forest_disturbances import data"
      ],
      "metadata": {
        "id": "h8n3VuivE2Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_REQUESTS = 20\n",
        "ASSET_PATH = \"projects/api-project-269347469410/assets/rylan-mssforestdisturbances/\""
      ],
      "metadata": {
        "id": "p2xpItp-hIeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Create a Covering Grid of Forest Dominated Canada"
      ],
      "metadata": {
        "id": "gR7Ys8nCJtAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.1\n",
        "\n",
        "Create a grid that covers all of forest dominated Canada, excluding cells that are >70% water. Export the resulting grid as an Earth Engine asset."
      ],
      "metadata": {
        "id": "Lrjolqzxe-4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRID_CELL_SIZE = 512\n",
        "grid = data.build_land_covering_grid(data.ECOZONES.geometry(), GRID_CELL_SIZE)\n",
        "grid_list = grid.toList(grid.size())\n",
        "ids = ee.List.sequence(0, grid.size().subtract(1))\n",
        "id_grid = ee.FeatureCollection(\n",
        "    ids.map(lambda i: ee.Feature(grid_list.get(i)).set('cell_id', i))\n",
        ")\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=id_grid,\n",
        "    description=\"export_land_covering_grid\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"land_covering_grid\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "tOXzAry5J1qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.2\n",
        "\n",
        "For each year for which we are generating training data estimate the amount of harvest and fire that occurred in each cell of the grid created in Step 1.1. Export the resulting FeatureCollection as an Earth Engine asset."
      ],
      "metadata": {
        "id": "I9BgkkWxeqiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_id(feature):\n",
        "    cell_id = feature.getNumber('cell_id').format(\"%d\")\n",
        "    year = feature.getNumber('year').format(\"%d\")\n",
        "    id = cell_id.cat('_').cat(year)\n",
        "    return feature.set(\"id\", id)\n",
        "\n",
        "base_grid = ee.FeatureCollection(os.path.join(ASSET_PATH, \"data\", \"land_covering_grid\"))\n",
        "\n",
        "for year in range(1985, 1996):\n",
        "    annual_grid = data.add_disturbance_counts(base_grid, year).map(set_id)\n",
        "\n",
        "    asset_name = f\"disturbance_estimate_grid_{year}\"\n",
        "    task = ee.batch.Export.table.toAsset(\n",
        "        collection=annual_grid,\n",
        "        description=f\"export_grid_with_disturbance_estimates_{year}\",\n",
        "        assetId=os.path.join(ASSET_PATH, \"data\", \"annual_grids\", asset_name)\n",
        "    )\n",
        "    task.start()"
      ],
      "metadata": {
        "id": "eQoIBRFYX_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Select Cells from Grid to Create Train/Test/Val Datasets"
      ],
      "metadata": {
        "id": "_eBdlxYZJ2zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annual_grids_assets = [\n",
        "    os.path.join(\n",
        "        ASSET_PATH,\n",
        "        \"data\",\n",
        "        \"annual_grids\",\n",
        "        f\"disturbance_estimate_grid_{year}\"\n",
        "    )\n",
        "    for year in range(1985, 1996)\n",
        "]\n",
        "annual_grids = ee.FeatureCollection([\n",
        "    ee.FeatureCollection(asset)\n",
        "    for asset in annual_grids_assets\n",
        "]).flatten()\n",
        "\n",
        "# perform the train/test/val splitting individually within each ecozone\n",
        "ecozones = annual_grids.aggregate_array(\"ecozone\").distinct().getInfo()\n",
        "ecozone_grids = [\n",
        "    annual_grids.filter(ee.Filter.eq(\"ecozone\", x))\n",
        "    for x in ecozones\n",
        "]\n",
        "\n",
        "cell_counts = [200, 200, 200]\n",
        "splits = [0.7, 0.15, 0.15]\n",
        "selected_cells = [\n",
        "    data.sample_cells(grid, *cell_counts, *splits)\n",
        "    for grid in ecozone_grids\n",
        "]\n",
        "\n",
        "# join the train/test/val groups from each ecozone\n",
        "# shuffle to ensure ecozones are intermingled\n",
        "train_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[0] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "test_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[1] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "val_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[2] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "\n",
        "# export each group to Google Earth Engine\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=train_cells,\n",
        "    description=\"export_train_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"train_cells\")\n",
        ")\n",
        "task.start()\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=test_cells,\n",
        "    description=\"export_test_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"test_cells\")\n",
        ")\n",
        "task.start()\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=val_cells,\n",
        "    description=\"export_val_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"val_cells\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "8A-YfrW3pce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Export Image Patches\n",
        "\n",
        "Based on https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/people-and-planet-ai/land-cover-classification\n",
        "and https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_computePixels.ipynb"
      ],
      "metadata": {
        "id": "xasxeHE6-S9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create default request for computePixels\n",
        "proj = data.PROJECTION.getInfo()\n",
        "scale_x = proj['transform'][0]\n",
        "scale_y = -proj['transform'][4]\n",
        "\n",
        "PATCH_SIZE = 512\n",
        "\n",
        "OFFSET_X = -scale_x * PATCH_SIZE / 2\n",
        "OFFSET_Y = -scale_y * PATCH_SIZE / 2\n",
        "\n",
        "REQUEST = {\n",
        "    'fileFormat': 'NPY',\n",
        "    'grid': {\n",
        "        'dimensions': {\n",
        "            'width': PATCH_SIZE,\n",
        "            'height': PATCH_SIZE,\n",
        "        },\n",
        "        'affineTransform': {\n",
        "            'scaleX': scale_x,\n",
        "            'shearX': 0,\n",
        "            'shearY': 0,\n",
        "            'scaleY': scale_y,\n",
        "        },\n",
        "        'crsCode': proj['wkt']\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "9xFbie6HGLyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ERROR_MARGIN = ee.ErrorMargin(0.1, \"projected\")\n",
        "\n",
        "def _get_images_from_feature(feature):\n",
        "    geom = feature.geometry(ERROR_MARGIN, data.PROJECTION)\n",
        "    year = feature.getNumber(\"year\")\n",
        "\n",
        "    # TODO: need to be able to handle this returning no images\n",
        "    images = msslib.getCol(\n",
        "        aoi=geom.centroid(1).buffer(60),\n",
        "        yearRange=[year, year],\n",
        "        doyRange=data.DOY_RANGE,\n",
        "        maxCloudCover=100\n",
        "    )\n",
        "\n",
        "    return images\n",
        "\n",
        "def get_image_ids(df, col, index):\n",
        "    cell_id = int(df.iloc[index][\"cell_id\"])\n",
        "    feature = col.filter(ee.Filter.eq(\"cell_id\", cell_id)).first()\n",
        "\n",
        "    images = _get_images_from_feature(feature)\n",
        "    image_ids = images.aggregate_array(\"LANDSAT_SCENE_ID\").getInfo()\n",
        "    return zip(image_ids, itertools.repeat(feature))\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def get_image_label_metadata(image_id, feature):\n",
        "    images = _get_images_from_feature(ee.Feature(feature))\n",
        "    image = images.filter(ee.Filter.eq(\"LANDSAT_SCENE_ID\", image_id)).first()\n",
        "\n",
        "    image, label = data.prepare_image_for_export(image)\n",
        "    metadata = data.prepare_metadata_for_export(image, feature)\n",
        "    metadata = {key: val.getInfo() for key, val in metadata.items()}\n",
        "\n",
        "    geom = feature.geometry(ERROR_MARGIN, data.PROJECTION)\n",
        "    coords = geom.centroid(ERROR_MARGIN, data.PROJECTION).getInfo()[\"coordinates\"]\n",
        "\n",
        "    return image.clip(geom), label.clip(geom), metadata\n",
        "\n",
        "    # request = dict(REQUEST)\n",
        "    # request['grid']['affineTransform']['translateX'] = coords[0] + OFFSET_X\n",
        "    # request['grid']['affineTransform']['translateY'] = coords[1] + OFFSET_Y\n",
        "\n",
        "    # image_request = dict(request)\n",
        "    # image_request['expression'] = image\n",
        "    # np_image = np.load(io.BytesIO(ee.data.computePixels(image_request)))\n",
        "\n",
        "    # label_request = dict(request)\n",
        "    # label_request['expression'] = label\n",
        "    # np_label = np.load(io.BytesIO(ee.data.computePixels(label_request)))\n",
        "\n",
        "    # return np_image, np_label, metadata\n",
        "\n",
        "\n",
        "def serialize_tensor(image, label, metadata):\n",
        "    features = {\n",
        "        key: tf.train.Feature(\n",
        "            int64_list=tf.train.Int64List(value=[value])\n",
        "        )\n",
        "        for key, value in metadata.items()\n",
        "    }\n",
        "\n",
        "    features['image'] = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(\n",
        "            value=[tf.io.serialize_tensor(image).numpy()]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    features['label'] = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(\n",
        "            value=[tf.io.serialize_tensor(label).numpy()]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "Map = geemap.Map()\n",
        "def add_to_map(image, label, metadata):\n",
        "    Map.addLayer(image)\n",
        "\n",
        "\n",
        "class ProcessSampleGroup(beam.PTransform):\n",
        "    def __init__(self, df, col, prefix):\n",
        "        super().__init__()\n",
        "        self.prefix = prefix\n",
        "        self.col = col\n",
        "        self.df = df\n",
        "\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | beam.FlatMap(lambda i: get_image_ids(self.df, self.col, i))\n",
        "            | beam.Reshuffle()\n",
        "            | beam.MapTuple(get_image_label_metadata)\n",
        "            | beam.MapTuple(add_to_map)\n",
        "            # | beam.MapTuple(serialize_tensor)\n",
        "            # | beam.io.WriteToTFRecord(self.prefix, file_name_suffix=\".tfrecord.gz\")\n",
        "        )\n",
        "\n",
        "def write_tfrecord(col, output_prefix):\n",
        "    df = geemap.ee_to_df(\n",
        "        col, col_names=['disturbance_type', 'ecozone', 'cell_id']\n",
        "    )\n",
        "    ecozones = set(df['ecozone'])\n",
        "    disturbance_types = set(df['disturbance_type'])\n",
        "\n",
        "    def filter_df(i, ecozone, disturbance_type):\n",
        "        elem = df.iloc[i]\n",
        "        matches_ecozone = elem['ecozone'] == ecozone\n",
        "        matches_disturbance_type = elem['disturbance_type'] == disturbance_type\n",
        "        return matches_ecozone and matches_disturbance_type\n",
        "\n",
        "    with beam.Pipeline() as pipeline:\n",
        "        pcoll = pipeline | beam.Create(df.index)\n",
        "\n",
        "        for ecozone in ecozones:\n",
        "            for disturbance_type in disturbance_types:\n",
        "                path = os.path.join(\n",
        "                    output_prefix,\n",
        "                    f\"ecozone{ecozone}\",\n",
        "                    disturbance_type\n",
        "                )\n",
        "\n",
        "                filter_label = f\"filter {ecozone} {disturbance_type}\"\n",
        "                inner_pcoll = pcoll | filter_label >> beam.Filter(\n",
        "                    lambda i: filter_df(i, ecozone, disturbance_type)\n",
        "                )\n",
        "\n",
        "                process_label = f\"process {ecozone} {disturbance_type}\"\n",
        "                inner_pcoll | process_label >> ProcessSampleGroup(\n",
        "                    prefix=path, df=df, col=col\n",
        "                )\n",
        "\n",
        "train_col = ee.FeatureCollection(os.path.join(ASSET_PATH, \"data\", \"train_cells\"))\n",
        "write_tfrecord(train_col.limit(1), os.path.join(\"scratch\", \"train\"))"
      ],
      "metadata": {
        "id": "gr9Jn2TEbZlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map"
      ],
      "metadata": {
        "id": "dBuY-fRUydua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}