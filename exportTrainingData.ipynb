{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnghG4wjWBPTLGxlufX4iG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/canadaMSSForestDisturbances/blob/main/exportTrainingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lHfGEmw8-PKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "UHuSQFE_TMlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -q --upgrade pip\n",
        "!pip install -q -q \"apache-beam[gcp]==2.50.0\"\n",
        "!pip install -q -q msslib"
      ],
      "metadata": {
        "id": "znBYs0werBFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMPH5MSEaA9_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import itertools\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "from google.api_core import retry\n",
        "\n",
        "import requests\n",
        "\n",
        "import ee\n",
        "import geemap\n",
        "import geopandas\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib import recfunctions as rfn\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from msslib import msslib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --quiet https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances\n",
        "from mss_forest_disturbances import constants, grid, preprocessing"
      ],
      "metadata": {
        "id": "h8n3VuivE2Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = constants.PROJECT\n",
        "!gcloud config set project {constants.PROJECT}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(\n",
        "    credentials,\n",
        "    project=constants.PROJECT,\n",
        "    opt_url=constants.HIGH_VOLUME_ENDPOINT\n",
        ")"
      ],
      "metadata": {
        "id": "tC8CmOUDa_sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Create a Covering Grid of Forest Dominated Canada"
      ],
      "metadata": {
        "id": "gR7Ys8nCJtAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.1\n",
        "\n",
        "Create a grid that covers all of forest dominated Canada, excluding cells that are >70% water. Export the resulting grid as an Earth Engine asset."
      ],
      "metadata": {
        "id": "Lrjolqzxe-4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "land_covering_grid = grid.build_land_covering_grid(\n",
        "    ee.FeatureCollection(constants.ECOZONES).geometry(),\n",
        "    constants.EXPORT_PATCH_SIZE\n",
        ")\n",
        "\n",
        "grid_list = land_covering_grid.toList(land_covering_grid.size())\n",
        "ids = ee.List.sequence(0, land_covering_grid.size().subtract(1))\n",
        "id_grid = ee.FeatureCollection(\n",
        "    ids.map(lambda i: ee.Feature(grid_list.get(i)).set('cell_id', i))\n",
        ")\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=id_grid,\n",
        "    description=\"export_land_covering_grid\",\n",
        "    assetId=os.path.join(constants.ASSET_PATH, \"data\", \"land_covering_grid\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "tOXzAry5J1qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.2\n",
        "\n",
        "For each year for which we are generating training data estimate the amount of harvest and fire that occurred in each cell of the grid created in Step 1.1. Export the resulting FeatureCollection as an Earth Engine asset."
      ],
      "metadata": {
        "id": "I9BgkkWxeqiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_id(feature):\n",
        "    cell_id = feature.getNumber('cell_id').format(\"%d\")\n",
        "    year = feature.getNumber('year').format(\"%d\")\n",
        "    id = cell_id.cat('_').cat(year)\n",
        "    return feature.set(\"id\", id)\n",
        "\n",
        "base_grid = ee.FeatureCollection(\n",
        "    os.path.join(constants.ASSET_PATH, \"data\", \"land_covering_grid\")\n",
        ")\n",
        "\n",
        "years = range(constants.FIRST_DISTURBANCE_YEAR, constants.LAST_MSS_YEAR + 1)\n",
        "for year in years:\n",
        "    annual_grid = grid.add_disturbance_counts(base_grid, year).map(set_id)\n",
        "\n",
        "    asset_name = f\"grid{year}\"\n",
        "    task = ee.batch.Export.table.toAsset(\n",
        "        collection=annual_grid,\n",
        "        description=f\"export_grid_with_disturbance_estimates_{year}\",\n",
        "        assetId=os.path.join(\n",
        "            constants.ASSET_PATH,\n",
        "            \"data\",\n",
        "            \"annual_grids\",\n",
        "            asset_name\n",
        "        )\n",
        "    )\n",
        "    task.start()"
      ],
      "metadata": {
        "id": "eQoIBRFYX_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.3\n",
        "\n",
        "Create a grid that covers all of forest dominated Canada, excluding no cells, with an overlap between adjacent cells of 8 pixels to avoid edge artifacts.\n",
        "\n",
        "We will use the grid from 1.1 to generate training data and this grid to create the final maps."
      ],
      "metadata": {
        "id": "WjzDRwHHqQ11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overlapped_grid = grid.build_grid(\n",
        "    ee.FeatureCollection(constants.ECOZONES).geometry(),\n",
        "    constants.PATCH_SIZE,\n",
        "    constants.OVERLAP\n",
        ")\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=overlapped_grid,\n",
        "    description='export_overlapped_grid',\n",
        "    assetId=os.path.join(constants.ASSET_PATH, \"data\", \"overlapped_grid\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "4lc-xVCHqnUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Select Cells from Grid to Create Train/Test/Val Datasets"
      ],
      "metadata": {
        "id": "_eBdlxYZJ2zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years = range(constants.FIRST_DISTURBANCE_YEAR, constants.LAST_MSS_YEAR + 1)\n",
        "annual_grids_assets = [\n",
        "    os.path.join(\n",
        "        constants.ASSET_PATH,\n",
        "        \"data\",\n",
        "        \"annual_grids\",\n",
        "        f\"grid{year}\"\n",
        "    )\n",
        "    for year in years\n",
        "]\n",
        "annual_grids = ee.FeatureCollection([\n",
        "    ee.FeatureCollection(asset)\n",
        "    for asset in annual_grids_assets\n",
        "]).flatten()\n",
        "\n",
        "covering_grid = ee.FeatureCollection(\n",
        "    os.path.join(constants.ASSET_PATH, \"data\", \"land_covering_grid\")\n",
        ")\n",
        "\n",
        "# perform the train/test/val splitting individually within each ecozone\n",
        "ecozones = annual_grids.aggregate_array(\"ecozone\").distinct()\n",
        "ecozone_grids = [\n",
        "    annual_grids.filter(ee.Filter.eq(\"ecozone\", x))\n",
        "    for x in ecozones.getInfo()\n",
        "]\n",
        "\n",
        "forested_ecozones = ee.FeatureCollection(constants.ECOZONES)\n",
        "total_forested_area = forested_ecozones.geometry().area()\n",
        "\n",
        "def calc_area(ecozone_id):\n",
        "    ecozone = forested_ecozones.filter(ee.Filter.eq(\"ECOZONE_ID\", ecozone_id))\n",
        "    return ecozone.geometry().area()\n",
        "\n",
        "ecozone_areas = ecozones.map(calc_area)\n",
        "ecozone_areas_percentage = ecozone_areas.map(\n",
        "    lambda x: ee.Number(x).divide(total_forested_area)\n",
        ").getInfo()\n",
        "\n",
        "# select 1200 fire, 1200 harvest, and 600 undisturbed cells in total\n",
        "# distributed across ecozones proportional to ecozone size\n",
        "cell_counts = np.array([1200, 1200, 600])\n",
        "splits = [0.7, 0.15, 0.15]\n",
        "selected_cells = [\n",
        "    grid.sample_cells(\n",
        "        ecozone_grid,\n",
        "        *np.ceil(cell_counts * percent).tolist(),\n",
        "        *splits\n",
        "    )\n",
        "    for ecozone_grid, percent in zip(ecozone_grids, ecozone_areas_percentage)\n",
        "]\n",
        "\n",
        "# join the train/test/val groups from each ecozone\n",
        "# shuffle to ensure ecozones are intermingled\n",
        "train_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[0] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "test_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[1] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "val_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[2] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "\n",
        "# get all the cells that never appear in train/test/val (regardless of year)\n",
        "used_cells_ids = ee.FeatureCollection([\n",
        "    train_cells, test_cells, val_cells\n",
        "]).flatten().aggregate_array(\"cell_id\")\n",
        "unused_cell_filter = ee.Filter.listContains(\n",
        "    rightField='cell_id',\n",
        "    leftValue=used_cells_ids,\n",
        ").Not()\n",
        "unused_cells = covering_grid.filter(unused_cell_filter)\n",
        "unused_cells = unused_cells.randomColumn(\"shuffle\", 42).sort(\"shuffle\")\n",
        "model2_train_cells = ee.FeatureCollection(unused_cells.toList(700))\n",
        "model2_test_cells = ee.FeatureCollection(unused_cells.toList(150, 700))\n",
        "model2_val_cells = ee.FeatureCollection(unused_cells.toList(150, 850))\n",
        "\n",
        "model1_cells = [train_cells, test_cells, val_cells]\n",
        "model2_cells = [model2_train_cells, model2_test_cells, model2_val_cells]\n",
        "for model, model_cells in zip([\"model1\", \"model2\"], [model1_cells, model2_cells]):\n",
        "    for group, cells in zip([\"train\", \"test\", \"val\"], model_cells):\n",
        "        path = os.path.join(constants.ASSET_PATH, \"data\", model, group)\n",
        "        task = ee.batch.Export.table.toAsset(\n",
        "            collection=cells,\n",
        "            description=\"export_cells\",\n",
        "            assetId=path,\n",
        "        )\n",
        "        task.start()"
      ],
      "metadata": {
        "id": "8A-YfrW3pce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Export Image Patches\n",
        "\n",
        "Based on https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/people-and-planet-ai/land-cover-classification\n",
        "and https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_computePixels.ipynb"
      ],
      "metadata": {
        "id": "xasxeHE6-S9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_uri = os.path.join(\n",
        "    constants.BASE_DOCKER_IMAGE_URI,\n",
        "    \"dataflow/dockerfile:1.0\"\n",
        ")"
      ],
      "metadata": {
        "id": "OjiRbFtxA0cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this only needs to be run to re/create the docker image!\n",
        "!gcloud builds submit --tag {image_uri} ."
      ],
      "metadata": {
        "id": "1_QaVxcx_sD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_location = os.path.join(constants.BUCKET, 'temp')\n",
        "staging_location = os.path.join(constants.BUCKET, 'staging')\n",
        "output_prefix = os.path.join(constants.BUCKET, 'scratch', 'test_export2')\n",
        "input_asset = os.path.join(constants.ASSET_PATH, 'data', 'train_cells')\n",
        "\n",
        "!python dataflow_job.py \\\n",
        "    --runner='DataflowRunner' \\\n",
        "    --project={constants.PROJECT} \\\n",
        "    --job_name='test-data-export' \\\n",
        "    --region='us-central1' \\\n",
        "    --temp_location={temp_location} \\\n",
        "    --staging_location={staging_location} \\\n",
        "    --num_workers=20 \\\n",
        "    --max-requests=20 \\\n",
        "    --input-asset={input_asset} \\\n",
        "    --output-prefix={output_prefix} \\\n",
        "    --experiments=use_runner_v2 \\\n",
        "    --sdk_container_image={image_uri} \\\n",
        "    --sdk_location=container"
      ],
      "metadata": {
        "id": "U53TLgaZ_koR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Verify TFRecords were Created Properly"
      ],
      "metadata": {
        "id": "hpE0yKvoFOQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FEATURES = {\n",
        "    b: tf.io.FixedLenFeature(\n",
        "        shape=[constants.EXPORT_PATCH_SIZE, constants.EXPORT_PATCH_SIZE],\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    for b in constants.BANDS\n",
        "}\n",
        "\n",
        "LABEL_FEATURES = {\n",
        "    \"label\": tf.io.FixedLenFeature(\n",
        "        shape=[constants.EXPORT_PATCH_SIZE, constants.EXPORT_PATCH_SIZE],\n",
        "        dtype=tf.int64\n",
        "        )\n",
        "}\n",
        "\n",
        "METADATA_FEATURES = {\n",
        "    m: tf.io.FixedLenFeature(shape=1, dtype=tf.int64)\n",
        "    for m in [\"ecozone\", \"doy\"]\n",
        "}\n",
        "\n",
        "def parse(example_proto):\n",
        "    image = tf.io.parse_single_example(example_proto, IMAGE_FEATURES)\n",
        "    metadata = tf.io.parse_single_example(example_proto, METADATA_FEATURES)\n",
        "    label = tf.io.parse_single_example(example_proto, LABEL_FEATURES)\n",
        "    return image, metadata, label\n",
        "\n",
        "files = tf.data.Dataset.list_files(f\"{output_prefix}/*/*.tfrecord.gz\")\n",
        "dataset = tf.data.TFRecordDataset(files, compression_type=\"GZIP\")\n",
        "dataset = dataset.map(parse, num_parallel_calls=5)\n",
        "\n",
        "for im, m, label in dataset.take(5):\n",
        "    im = tf.stack([im[b] for b in constants.BANDS], axis=-1)\n",
        "    label = label[\"label\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, squeeze=True)\n",
        "    axes[0].imshow(im[:, :, :3], vmin=0.02, vmax=0.08)\n",
        "    axes[1].imshow(label)\n",
        "    plt.show()\n",
        "    print([(k, v.numpy()) for k, v in m.items()])\n"
      ],
      "metadata": {
        "id": "xThhpFKQFSGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}