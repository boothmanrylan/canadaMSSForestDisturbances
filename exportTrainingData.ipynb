{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGQ8hjUVqO+LJHpGUwv7UB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/canadaMSSForestDisturbances/blob/main/exportTrainingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lHfGEmw8-PKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "UHuSQFE_TMlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -q --upgrade pip\n",
        "!pip install -q -q \"apache-beam[gcp]==2.50.0\"\n",
        "!pip install -q -q geemap\n",
        "!pip install -q -q msslib"
      ],
      "metadata": {
        "id": "znBYs0werBFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMPH5MSEaA9_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import itertools\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "from google.api_core import retry\n",
        "\n",
        "import requests\n",
        "\n",
        "import ee\n",
        "import geemap\n",
        "import geopandas\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib import recfunctions as rfn\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = 'api-project-269347469410'\n",
        "BUCKET = 'gs://rylan-mssforestdisturbances/'\n",
        "LOCATION = 'us-central1'\n",
        "\n",
        "HIGH_VOLUME_ENDPOINT = 'https://earthengine-highvolume.googleapis.com'\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT\n",
        "!gcloud config set project {PROJECT}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT, opt_url=HIGH_VOLUME_ENDPOINT)\n",
        "\n",
        "from msslib import msslib"
      ],
      "metadata": {
        "id": "tC8CmOUDa_sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --quiet https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances\n",
        "from mss_forest_disturbances import data"
      ],
      "metadata": {
        "id": "h8n3VuivE2Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_REQUESTS = 20\n",
        "ASSET_PATH = \"projects/api-project-269347469410/assets/rylan-mssforestdisturbances/\""
      ],
      "metadata": {
        "id": "p2xpItp-hIeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Create a Covering Grid of Forest Dominated Canada"
      ],
      "metadata": {
        "id": "gR7Ys8nCJtAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.1\n",
        "\n",
        "Create a grid that covers all of forest dominated Canada, excluding cells that are >70% water. Export the resulting grid as an Earth Engine asset."
      ],
      "metadata": {
        "id": "Lrjolqzxe-4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRID_CELL_SIZE = 512\n",
        "grid = data.build_land_covering_grid(\n",
        "    ee.FeatureCollection(data.ECOZONES).geometry(),\n",
        "    GRID_CELL_SIZE\n",
        ")\n",
        "grid_list = grid.toList(grid.size())\n",
        "ids = ee.List.sequence(0, grid.size().subtract(1))\n",
        "id_grid = ee.FeatureCollection(\n",
        "    ids.map(lambda i: ee.Feature(grid_list.get(i)).set('cell_id', i))\n",
        ")\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=id_grid,\n",
        "    description=\"export_land_covering_grid\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"land_covering_grid\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "tOXzAry5J1qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.2\n",
        "\n",
        "For each year for which we are generating training data estimate the amount of harvest and fire that occurred in each cell of the grid created in Step 1.1. Export the resulting FeatureCollection as an Earth Engine asset."
      ],
      "metadata": {
        "id": "I9BgkkWxeqiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_id(feature):\n",
        "    cell_id = feature.getNumber('cell_id').format(\"%d\")\n",
        "    year = feature.getNumber('year').format(\"%d\")\n",
        "    id = cell_id.cat('_').cat(year)\n",
        "    return feature.set(\"id\", id)\n",
        "\n",
        "base_grid = ee.FeatureCollection(os.path.join(ASSET_PATH, \"data\", \"land_covering_grid\"))\n",
        "\n",
        "for year in range(1985, 1996):\n",
        "    annual_grid = data.add_disturbance_counts(base_grid, year).map(set_id)\n",
        "\n",
        "    asset_name = f\"disturbance_estimate_grid_{year}\"\n",
        "    task = ee.batch.Export.table.toAsset(\n",
        "        collection=annual_grid,\n",
        "        description=f\"export_grid_with_disturbance_estimates_{year}\",\n",
        "        assetId=os.path.join(ASSET_PATH, \"data\", \"annual_grids\", asset_name)\n",
        "    )\n",
        "    task.start()"
      ],
      "metadata": {
        "id": "eQoIBRFYX_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Select Cells from Grid to Create Train/Test/Val Datasets"
      ],
      "metadata": {
        "id": "_eBdlxYZJ2zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annual_grids_assets = [\n",
        "    os.path.join(\n",
        "        ASSET_PATH,\n",
        "        \"data\",\n",
        "        \"annual_grids\",\n",
        "        f\"disturbance_estimate_grid_{year}\"\n",
        "    )\n",
        "    for year in range(1985, 1996)\n",
        "]\n",
        "annual_grids = ee.FeatureCollection([\n",
        "    ee.FeatureCollection(asset)\n",
        "    for asset in annual_grids_assets\n",
        "]).flatten()\n",
        "\n",
        "# perform the train/test/val splitting individually within each ecozone\n",
        "ecozones = annual_grids.aggregate_array(\"ecozone\").distinct()\n",
        "ecozone_grids = [\n",
        "    annual_grids.filter(ee.Filter.eq(\"ecozone\", x))\n",
        "    for x in ecozones.getInfo()\n",
        "]\n",
        "\n",
        "forested_ecozones = ee.FeatureCollection(\n",
        "    \"users/boothmanrylan/forest_dominated_ecozones\"\n",
        ")\n",
        "total_forested_area = forested_ecozones.geometry().area()\n",
        "\n",
        "def calc_area(ecozone_id):\n",
        "    ecozone = forested_ecozones.filter(ee.Filter.eq(\"ECOZONE_ID\", ecozone_id))\n",
        "    return ecozone.geometry().area()\n",
        "\n",
        "ecozone_areas = ecozones.map(calc_area)\n",
        "ecozone_areas_percentage = ecozone_areas.map(\n",
        "    lambda x: ee.Number(x).divide(total_forested_area)\n",
        ")\n",
        "\n",
        "# select 1000 fire, 1000 harvest, and 500 undisturbed cells in total\n",
        "# distributed across ecozones proportional to ecozone size\n",
        "\n",
        "cell_counts = np.array([1000, 1000, 500])\n",
        "splits = [0.7, 0.15, 0.15]\n",
        "selected_cells = [\n",
        "    data.sample_cells(grid, *np.ceil(cell_counts * percent).tolist(), *splits)\n",
        "    for grid, percent in zip(ecozone_grids, ecozone_areas_percentage.getInfo())\n",
        "]\n",
        "\n",
        "# join the train/test/val groups from each ecozone\n",
        "# shuffle to ensure ecozones are intermingled\n",
        "train_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[0] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "test_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[1] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "val_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[2] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "\n",
        "# export each group to Google Earth Engine\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=train_cells,\n",
        "    description=\"export_train_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"train_cells\")\n",
        ")\n",
        "task.start()\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=test_cells,\n",
        "    description=\"export_test_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"test_cells\")\n",
        ")\n",
        "task.start()\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=val_cells,\n",
        "    description=\"export_val_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"val_cells\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "8A-YfrW3pce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images(feat):\n",
        "    year = feat.getNumber(\"year\")\n",
        "    geom = feat.geometry()\n",
        "    centroid = geom.centroid(1)\n",
        "\n",
        "    images = msslib.getCol(\n",
        "        aoi=centroid,\n",
        "        yearRange=[year, year],\n",
        "        doyRange=data.DOY_RANGE,\n",
        "        maxCloudCover=100\n",
        "    )\n",
        "    return feat.set(\"num_images\", images.size())\n",
        "\n",
        "train_cells = ee.FeatureCollection(\n",
        "    os.path.join(ASSET_PATH, \"data\", \"val_cells\")\n",
        ")\n",
        "train_cells = train_cells.map(count_images)\n",
        "filtered_cells = train_cells.filter(ee.Filter.eq(\"num_images\", 0))\n",
        "print(train_cells.size().getInfo(), filtered_cells.size().getInfo())"
      ],
      "metadata": {
        "id": "h83Wdd55KQD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Export Image Patches\n",
        "\n",
        "Based on https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/people-and-planet-ai/land-cover-classification\n",
        "and https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_computePixels.ipynb"
      ],
      "metadata": {
        "id": "xasxeHE6-S9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_uri = f\"us-central1-docker.pkg.dev/{PROJECT}/dataflow-containers/dataflow/dockerfile:1.0\""
      ],
      "metadata": {
        "id": "OjiRbFtxA0cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this only needs to be run once to create the docker image in the artifact registry\n",
        "!gcloud builds submit --tag {image_uri} ."
      ],
      "metadata": {
        "id": "1_QaVxcx_sD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_location = os.path.join(BUCKET, 'temp')\n",
        "staging_location = os.path.join(BUCKET, 'staging')\n",
        "output_prefix = os.path.join(BUCKET, 'scratch', 'test_export2')\n",
        "input_asset = os.path.join(ASSET_PATH, 'data', 'train_cells')\n",
        "\n",
        "!python dataflow_job.py \\\n",
        "    --runner='DataflowRunner' \\\n",
        "    --project='{PROJECT}' \\\n",
        "    --job_name='test-data-export' \\\n",
        "    --region='us-central1' \\\n",
        "    --temp_location='{temp_location}' \\\n",
        "    --staging_location='{staging_location}' \\\n",
        "    --num_workers=20 \\\n",
        "    --max-requests=20 \\\n",
        "    --input-asset='{input_asset}' \\\n",
        "    --output-prefix='{output_prefix}' \\\n",
        "    --experiments=use_runner_v2 \\\n",
        "    --sdk_container_image='{image_uri}' \\\n",
        "    --sdk_location=container"
      ],
      "metadata": {
        "id": "U53TLgaZ_koR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Verify TFRecords were Created Properly"
      ],
      "metadata": {
        "id": "hpE0yKvoFOQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataflow_job import BANDS\n",
        "\n",
        "IMAGE_FEATURES = {\n",
        "    b: tf.io.FixedLenFeature(shape=[512, 512], dtype=tf.float32)\n",
        "    for b in BANDS\n",
        "}\n",
        "\n",
        "LABEL_FEATURES = {\n",
        "    \"label\": tf.io.FixedLenFeature(shape=[512, 512], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "METADATA_FEATURES = {\n",
        "    m: tf.io.FixedLenFeature(shape=1, dtype=tf.int64)\n",
        "    for m in [\"ecozone\", \"doy\"]\n",
        "}\n",
        "\n",
        "def parse(example_proto):\n",
        "    image = tf.io.parse_single_example(example_proto, IMAGE_FEATURES)\n",
        "    metadata = tf.io.parse_single_example(example_proto, METADATA_FEATURES)\n",
        "    label = tf.io.parse_single_example(example_proto, LABEL_FEATURES)\n",
        "    return image, metadata, label\n",
        "\n",
        "files = tf.data.Dataset.list_files(f\"{output_prefix}/*/*.tfrecord.gz\")\n",
        "dataset = tf.data.TFRecordDataset(files, compression_type=\"GZIP\")\n",
        "dataset = dataset.map(parse, num_parallel_calls=5)\n",
        "\n",
        "for im, m, label in dataset.take(5):\n",
        "    im = tf.stack([im[b] for b in BANDS], axis=-1)\n",
        "    label = label[\"label\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, squeeze=True)\n",
        "    axes[0].imshow(im[:, :, :3], vmin=0.02, vmax=0.08)\n",
        "    axes[1].imshow(label)\n",
        "    plt.show()\n",
        "    print([(k, v.numpy()) for k, v in m.items()])\n"
      ],
      "metadata": {
        "id": "xThhpFKQFSGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}