{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOSTKKErZAvtR0vQSHyqgtj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/canadaMSSForestDisturbances/blob/main/exportTrainingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lHfGEmw8-PKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "UHuSQFE_TMlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade pip\n",
        "!pip install --quiet \"apache-beam[gcp]==2.46.0\"\n",
        "!pip install --quiet geemap\n",
        "!pip install --quiet msslib"
      ],
      "metadata": {
        "id": "znBYs0werBFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMPH5MSEaA9_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import itertools\n",
        "\n",
        "import google\n",
        "from google.colab import auth\n",
        "from google.api_core import retry\n",
        "\n",
        "import requests\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "import ee\n",
        "import geemap\n",
        "import geopandas\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib import recfunctions as rfn\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = 'api-project-269347469410'\n",
        "BUCKET = 'gs://rylan-mssforestdisturbances/'\n",
        "LOCATION = 'us-central1'\n",
        "\n",
        "HIGH_VOLUME_ENDPOINT = 'https://earthengine-highvolume.googleapis.com'\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT\n",
        "!gcloud config set project {PROJECT}\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT, opt_url=HIGH_VOLUME_ENDPOINT)\n",
        "\n",
        "from msslib import msslib"
      ],
      "metadata": {
        "id": "tC8CmOUDa_sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --quiet https://github.com/boothmanrylan/canadaMSSForestDisturbances.git\n",
        "%cd canadaMSSForestDisturbances\n",
        "from mss_forest_disturbances import data"
      ],
      "metadata": {
        "id": "h8n3VuivE2Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_REQUESTS = 20\n",
        "ASSET_PATH = \"projects/api-project-269347469410/assets/rylan-mssforestdisturbances/\""
      ],
      "metadata": {
        "id": "p2xpItp-hIeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Create a Covering Grid of Forest Dominated Canada"
      ],
      "metadata": {
        "id": "gR7Ys8nCJtAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.1\n",
        "\n",
        "Create a grid that covers all of forest dominated Canada, excluding cells that are >70% water. Export the resulting grid as an Earth Engine asset."
      ],
      "metadata": {
        "id": "Lrjolqzxe-4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRID_CELL_SIZE = 512\n",
        "grid = data.build_land_covering_grid(data.ECOZONES.geometry(), GRID_CELL_SIZE)\n",
        "grid_list = grid.toList(grid.size())\n",
        "ids = ee.List.sequence(0, grid.size().subtract(1))\n",
        "id_grid = ee.FeatureCollection(\n",
        "    ids.map(lambda i: ee.Feature(grid_list.get(i)).set('cell_id', i))\n",
        ")\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=id_grid,\n",
        "    description=\"export_land_covering_grid\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"land_covering_grid\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "tOXzAry5J1qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1.2\n",
        "\n",
        "For each year for which we are generating training data estimate the amount of harvest and fire that occurred in each cell of the grid created in Step 1.1. Export the resulting FeatureCollection as an Earth Engine asset."
      ],
      "metadata": {
        "id": "I9BgkkWxeqiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_id(feature):\n",
        "    cell_id = feature.getNumber('cell_id').format(\"%d\")\n",
        "    year = feature.getNumber('year').format(\"%d\")\n",
        "    id = cell_id.cat('_').cat(year)\n",
        "    return feature.set(\"id\", id)\n",
        "\n",
        "base_grid = ee.FeatureCollection(os.path.join(ASSET_PATH, \"data\", \"land_covering_grid\"))\n",
        "\n",
        "for year in range(1985, 1996):\n",
        "    annual_grid = data.add_disturbance_counts(base_grid, year).map(set_id)\n",
        "\n",
        "    asset_name = f\"disturbance_estimate_grid_{year}\"\n",
        "    task = ee.batch.Export.table.toAsset(\n",
        "        collection=annual_grid,\n",
        "        description=f\"export_grid_with_disturbance_estimates_{year}\",\n",
        "        assetId=os.path.join(ASSET_PATH, \"data\", \"annual_grids\", asset_name)\n",
        "    )\n",
        "    task.start()"
      ],
      "metadata": {
        "id": "eQoIBRFYX_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Select Cells from Grid to Create Train/Test/Val Datasets"
      ],
      "metadata": {
        "id": "_eBdlxYZJ2zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annual_grids_assets = [\n",
        "    os.path.join(\n",
        "        ASSET_PATH,\n",
        "        \"data\",\n",
        "        \"annual_grids\",\n",
        "        f\"disturbance_estimate_grid_{year}\"\n",
        "    )\n",
        "    for year in range(1985, 1996)\n",
        "]\n",
        "annual_grids = ee.FeatureCollection([\n",
        "    ee.FeatureCollection(asset)\n",
        "    for asset in annual_grids_assets\n",
        "]).flatten()\n",
        "\n",
        "# perform the train/test/val splitting individually within each ecozone\n",
        "ecozones = annual_grids.aggregate_array(\"ecozone\").distinct().getInfo()\n",
        "ecozone_grids = [\n",
        "    annual_grids.filter(ee.Filter.eq(\"ecozone\", x))\n",
        "    for x in ecozones\n",
        "]\n",
        "\n",
        "cell_counts = [200, 200, 200]\n",
        "splits = [0.7, 0.15, 0.15]\n",
        "selected_cells = [\n",
        "    data.sample_cells(grid, *cell_counts, *splits)\n",
        "    for grid in ecozone_grids\n",
        "]\n",
        "\n",
        "# join the train/test/val groups from each ecozone\n",
        "# shuffle to ensure ecozones are intermingled\n",
        "train_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[0] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "test_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[1] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "val_cells = ee.FeatureCollection(\n",
        "    [ecozone_selection[2] for ecozone_selection in selected_cells]\n",
        ").flatten().sort(\"shuffle\")\n",
        "\n",
        "# export each group to Google Earth Engine\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=train_cells,\n",
        "    description=\"export_train_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"train_cells\")\n",
        ")\n",
        "task.start()\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=test_cells,\n",
        "    description=\"export_test_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"test_cells\")\n",
        ")\n",
        "task.start()\n",
        "\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=val_cells,\n",
        "    description=\"export_val_cells\",\n",
        "    assetId=os.path.join(ASSET_PATH, \"data\", \"val_cells\")\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "8A-YfrW3pce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images(feat):\n",
        "    year = feat.getNumber(\"year\")\n",
        "    geom = feat.geometry(ERROR_MARGIN, PROJECTION)\n",
        "    centroid = geom.centroid(1)\n",
        "\n",
        "    images = msslib.getCol(\n",
        "        aoi=centroid,\n",
        "        yearRange=[year, year],\n",
        "        doyRange=data.DOY_RANGE,\n",
        "        maxCloudCover=100\n",
        "    )\n",
        "    return feat.set(\"num_images\", images.size())\n",
        "\n",
        "train_cells = ee.FeatureCollection(\n",
        "    os.path.join(ASSET_PATH, \"data\", \"val_cells\")\n",
        ")\n",
        "train_cells = train_cells.map(count_images)\n",
        "filtered_cells = train_cells.filter(ee.Filter.eq(\"num_images\", 0))\n",
        "print(train_cells.size().getInfo(), filtered_cells.size().getInfo())"
      ],
      "metadata": {
        "id": "h83Wdd55KQD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Export Image Patches\n",
        "\n",
        "Based on https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/people-and-planet-ai/land-cover-classification\n",
        "and https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_computePixels.ipynb"
      ],
      "metadata": {
        "id": "xasxeHE6-S9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create default request for computePixels\n",
        "# proj = data.PROJECTION.getInfo()\n",
        "PROJECTION = ee.Projection('EPSG:4269').atScale(60)\n",
        "proj = PROJECTION.getInfo()\n",
        "scale_x = proj['transform'][0]\n",
        "scale_y = -proj['transform'][4]\n",
        "\n",
        "PATCH_SIZE = 512\n",
        "\n",
        "OFFSET_X = -scale_x * PATCH_SIZE / 2\n",
        "OFFSET_Y = -scale_y * PATCH_SIZE / 2\n",
        "\n",
        "REQUEST = {\n",
        "    'fileFormat': 'NPY',\n",
        "    'grid': {\n",
        "        'dimensions': {\n",
        "            'width': PATCH_SIZE,\n",
        "            'height': PATCH_SIZE,\n",
        "        },\n",
        "        'affineTransform': {\n",
        "            'scaleX': scale_x,\n",
        "            'shearX': 0,\n",
        "            'shearY': 0,\n",
        "            'scaleY': scale_y,\n",
        "        },\n",
        "        'crsCode': proj['crs']\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "9xFbie6HGLyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bands = ['nir', 'red_edge', 'red', 'green', 'tca', 'ndvi']\n",
        "historical_bands = ['historical_' + x for x in bands]\n",
        "BANDS = bands + historical_bands\n",
        "\n",
        "ERROR_MARGIN = ee.ErrorMargin(0.1, \"projected\")\n",
        "\n",
        "\n",
        "def ee_init():\n",
        "    credentials, project = google.auth.default(\n",
        "        scopes=[\n",
        "            \"https://www.googleapis.com/auth/cloud-platform\",\n",
        "            \"https://www.googleapis.com/auth/earthengine\",\n",
        "        ]\n",
        "    )\n",
        "    ee.Initialize(\n",
        "        credentials.with_quota_project(None),\n",
        "        project=project,\n",
        "        opt_url=HIGH_VOLUME_ENDPOINT,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_images_from_feature(feature):\n",
        "    geom = feature.geometry(ERROR_MARGIN, PROJECTION)\n",
        "    year = feature.getNumber(\"year\")\n",
        "\n",
        "    images = msslib.getCol(\n",
        "        aoi=geom.centroid(1),\n",
        "        yearRange=[year, year],\n",
        "        doyRange=data.DOY_RANGE,\n",
        "        maxCloudCover=100\n",
        "    )\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def get_image_ids(row, asset_path):\n",
        "    ee_init()\n",
        "\n",
        "    col = ee.FeatureCollection(asset_path)\n",
        "    feature = col.filter(ee.Filter.eq(\"id\", row[\"id\"])).first()\n",
        "\n",
        "    images = _get_images_from_feature(feature)\n",
        "\n",
        "    image_ids = images.aggregate_array(\"system:id\").getInfo()\n",
        "    feature_ids = itertools.repeat(row[\"id\"])\n",
        "    paths = itertools.repeat(asset_path)\n",
        "\n",
        "    return zip(image_ids, feature_ids, paths)\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def get_image_label_metadata(image_id, feature_id, asset_path):\n",
        "    ee_init()\n",
        "\n",
        "    image = msslib.process(ee.Image(image_id))\n",
        "    image, label = data.prepare_image_for_export(image)\n",
        "    image = image.select(BANDS)\n",
        "\n",
        "    col = ee.FeatureCollection(asset_path)\n",
        "    feature = col.filter(ee.Filter.eq(\"id\", feature_id)).first()\n",
        "    metadata = data.prepare_metadata_for_export(image, feature)\n",
        "    metadata = {key: val.getInfo() for key, val in metadata.items()}\n",
        "\n",
        "    geom = feature.geometry(ERROR_MARGIN, PROJECTION)\n",
        "    coords = geom.centroid(1).getInfo()[\"coordinates\"]\n",
        "\n",
        "    request = dict(REQUEST)\n",
        "    request['grid']['affineTransform']['translateX'] = coords[0] + OFFSET_X\n",
        "    request['grid']['affineTransform']['translateY'] = coords[1] + OFFSET_Y\n",
        "\n",
        "    image_request = dict(request)\n",
        "    image_request['expression'] = image.unmask(0)\n",
        "    np_image = np.load(io.BytesIO(ee.data.computePixels(image_request)))\n",
        "\n",
        "    label_request = dict(request)\n",
        "    label_request['expression'] = label.unmask(0)\n",
        "    np_label = np.load(io.BytesIO(ee.data.computePixels(label_request)))\n",
        "\n",
        "    return np_image, np_label, metadata\n",
        "\n",
        "\n",
        "def serialize_tensor(image, label, metadata):\n",
        "    features  = {\n",
        "        b: tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(\n",
        "                value=image[b].flatten()\n",
        "            )\n",
        "        )\n",
        "        for b in BANDS\n",
        "    }\n",
        "\n",
        "    features[\"label\"] = tf.train.Feature(\n",
        "        int64_list=tf.train.Int64List(\n",
        "            value=label[\"label\"].flatten()\n",
        "        )\n",
        "    )\n",
        "\n",
        "    for key, value in metadata.items():\n",
        "        features[key] = tf.train.Feature(\n",
        "            int64_list=tf.train.Int64List(value=[value])\n",
        "        )\n",
        "\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "def write_tfrecord(input_asset_path, output_prefix, pipeline_options=None):\n",
        "    col = ee.FeatureCollection(input_asset_path)\n",
        "    df = geemap.ee_to_df(\n",
        "        col, col_names=['disturbance_type', 'ecozone', 'id', \"shuffle\"]\n",
        "    )\n",
        "\n",
        "    ########################################################\n",
        "    # work on a small random subset of the complete dataframe\n",
        "    df = df.sort_values(by=\"shuffle\", ignore_index=True).head(20)\n",
        "    ########################################################\n",
        "\n",
        "    ecozones = set(df['ecozone'])\n",
        "    disturbance_types = set(df['disturbance_type'])\n",
        "\n",
        "    sets = list(itertools.product(ecozones, disturbance_types))\n",
        "    paths = [\n",
        "        os.path.join(output_prefix, f\"ecozone{ecozone}\", disturbance_type)\n",
        "        for ecozone, disturbance_type in sets\n",
        "    ]\n",
        "\n",
        "    def partition(elem, _num_partitions):\n",
        "        elem_set = (int(elem[\"ecozone\"]), elem[\"disturbance_type\"])\n",
        "        return sets.index(elem_set)\n",
        "\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        pcoll = pipeline | beam.Create(list(df.iloc))  # iloc cannot be directly pickled\n",
        "        groups = pcoll | beam.Partition(partition, len(sets))\n",
        "\n",
        "        for i, group in enumerate(groups):\n",
        "            uid = f\"{sets[i][0]}_{sets[i][1]}\"\n",
        "            (group\n",
        "             | f\"{uid} get ids\" >> beam.FlatMap(get_image_ids, asset_path=input_asset_path)\n",
        "             | f\"{uid} reshuffle\" >> beam.Reshuffle()\n",
        "             | f\"{uid} get data\" >> beam.MapTuple(get_image_label_metadata)\n",
        "             | f\"{uid} serialize\" >> beam.MapTuple(serialize_tensor)\n",
        "             | f\"{uid} write\" >> beam.io.WriteToTFRecord(paths[i], file_name_suffix=\".tfrecord.gz\")\n",
        "            )\n",
        "\n",
        "pipeline_options = PipelineOptions(\n",
        "    runner=\"DataflowRunner\",\n",
        "    project=PROJECT,\n",
        "    job_name=\"test-data-export-workflow\",\n",
        "    region=\"us-central1\",\n",
        "    save_main_session=True,\n",
        "    setup_file=\"./setup.py\",\n",
        "    max_num_workers=20,\n",
        "    disk_size_gb=50,\n",
        "    temp_location=os.path.join(BUCKET, \"temp\"),\n",
        ")\n",
        "# pipeline_options = None\n",
        "\n",
        "train_col_asset_path = os.path.join(ASSET_PATH, \"data\", \"train_cells\")\n",
        "write_tfrecord(\n",
        "    train_col_asset_path,\n",
        "    os.path.join(BUCKET, \"scratch\", \"test_export\"),\n",
        "    pipeline_options,\n",
        ")"
      ],
      "metadata": {
        "id": "gr9Jn2TEbZlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = ee.FeatureCollection(train_col_asset_path)\n",
        "feature = collection.first()\n",
        "images = msslib.getCol(\n",
        "    aoi=feature.geometry().centroid(1),\n",
        "    yearRange=[feature.getNumber(\"year\"), feature.getNumber(\"year\")],\n",
        "    doyRange=data.DOY_RANGE,\n",
        "    maxCloudCover=100\n",
        ")\n",
        "image = images.first()\n",
        "\n",
        "system_id = image.get('system:id').getInfo()\n",
        "\n",
        "test_image = ee.Image(system_id)\n",
        "print(test_image.getInfo)"
      ],
      "metadata": {
        "id": "mQ5dwe0m9En1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Verify TFRecords were Created Properly"
      ],
      "metadata": {
        "id": "hpE0yKvoFOQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FEATURES = {\n",
        "    b: tf.io.FixedLenFeature(shape=[512, 512], dtype=tf.float32)\n",
        "    for b in BANDS\n",
        "}\n",
        "\n",
        "LABEL_FEATURES = {\n",
        "    \"label\": tf.io.FixedLenFeature(shape=[512, 512], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "METADATA_FEATURES = {\n",
        "    m: tf.io.FixedLenFeature(shape=1, dtype=tf.int64)\n",
        "    for m in [\"ecozone\", \"doy\"]\n",
        "}\n",
        "\n",
        "def parse(example_proto):\n",
        "    image = tf.io.parse_single_example(example_proto, IMAGE_FEATURES)\n",
        "    metadata = tf.io.parse_single_example(example_proto, METADATA_FEATURES)\n",
        "    label = tf.io.parse_single_example(example_proto, LABEL_FEATURES)\n",
        "    return image, metadata, label\n",
        "\n",
        "files = tf.data.Dataset.list_files(\"scratch/train/ecozone4/fire*.tfrecord.gz\")\n",
        "dataset = tf.data.TFRecordDataset(files, compression_type=\"GZIP\")\n",
        "dataset = dataset.map(parse, num_parallel_calls=5)\n",
        "\n",
        "for im, m, label in dataset.take(5):\n",
        "    im = tf.stack([im[b] for b in BANDS], axis=-1)\n",
        "    label = label[\"label\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, squeeze=True)\n",
        "    axes[0].imshow(im[:, :, :3], vmin=0.02, vmax=0.08)\n",
        "    axes[1].imshow(label)\n",
        "    plt.show()\n",
        "    print([(k, v.numpy()) for k, v in m.items()])\n"
      ],
      "metadata": {
        "id": "xThhpFKQFSGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}